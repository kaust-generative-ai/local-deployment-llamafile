{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a68b39c-ffc5-481c-a569-406be0e26496",
   "metadata": {},
   "source": [
    "## Using llamafile with external weights\n",
    "\n",
    "Even though our example llamafiles have the weights built-in, you don't *have* to use llamafile that way. Instead, you can [download](https://github.com/Mozilla-Ocho/llamafile/releases) *just* the llamafile software (without any weights included). You can then use it alongside any external weights you may have on hand.\n",
    "\n",
    "External weights are particularly useful for Windows users because they enable you to work around Windows' 4GB executable file size limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dee7aa-7757-4289-815b-3281cd0a3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Documents/Training/kaust-generative-ai/local-deployment-template/env/bin/llamafile-0.8.13\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "which llamafile-0.8.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63373f9d-e203-40a8-baa5-199ae8489828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mLLAMAFILE\u001b[24m(1)                General Commands Manual               \u001b[4mLLAMAFILE\u001b[24m(1)\n",
      "\n",
      "\u001b[1mNAME\u001b[0m\n",
      "       llamafile — large language model runner\n",
      "\n",
      "\u001b[1mSYNOPSIS\u001b[0m\n",
      "       \u001b[1mllamafile \u001b[22m[\u001b[1m--server\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m [\u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m]\n",
      "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\n",
      "       \u001b[1mllamafile \u001b[22m[\u001b[1m--cli\u001b[22m] [flags...] \u001b[1m-m \u001b[4m\u001b[22mmodel.gguf\u001b[24m \u001b[1m--mmproj \u001b[4m\u001b[22mvision.gguf\u001b[24m \u001b[1m--image\u001b[0m\n",
      "                 \u001b[4mgraphic.png\u001b[24m \u001b[1m-p \u001b[4m\u001b[22mprompt\u001b[0m\n",
      "\n",
      "\u001b[1mDESCRIPTION\u001b[0m\n",
      "       \u001b[1mllamafile \u001b[22mis a large language model tool. It has use cases such as:\n",
      "\n",
      "       \u001b[1m-   \u001b[22mCode completion\n",
      "       \u001b[1m-   \u001b[22mProse composition\n",
      "       \u001b[1m-   \u001b[22mChatbot that passes the Turing test\n",
      "       \u001b[1m-   \u001b[22mText/image summarization and analysis\n",
      "\n",
      "\u001b[1mOPTIONS\u001b[0m\n",
      "       The following options are available:\n",
      "\n",
      "       \u001b[1m--version\u001b[0m\n",
      "               Print version and exit.\n",
      "\n",
      "       \u001b[1m-h\u001b[22m, \u001b[1m--help\u001b[0m\n",
      "               Show help message and exit.\n",
      "\n",
      "       \u001b[1m--cli   \u001b[22mPuts  program  in command line interface mode. This flag is im‐\n",
      "               plied when a prompt is supplied  using  either  the  \u001b[1m-p  \u001b[22mor  \u001b[1m-f\u001b[0m\n",
      "               flags.\n",
      "\n",
      "       \u001b[1m--server\u001b[0m\n",
      "               Puts program in server mode. This will launch an HTTP server on\n",
      "               a  local  port. This server has both a web UI and an OpenAI API\n",
      "               compatible completions endpoint. When the server is  run  on  a\n",
      "               desk  system,  a tab browser tab will be launched automatically\n",
      "               that displays the web UI.  This \u001b[1m--server \u001b[22mflag is implied if  no\n",
      "               prompt  is  specified,  i.e.  neither  the  \u001b[1m-p  \u001b[22mor \u001b[1m-f \u001b[22mflags are\n",
      "               passed.\n",
      "\n",
      "       \u001b[1m-m \u001b[4m\u001b[22mFNAME\u001b[24m, \u001b[1m--model \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Model path in the GGUF file format.\n",
      "\n",
      "               Default: \u001b[4mmodels/7B/ggml-model-f16.gguf\u001b[0m\n",
      "\n",
      "       \u001b[1m--mmproj \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Specifies path of the LLaVA vision model in the GGUF file  for‐\n",
      "               mat.  If  this  flag  is supplied, then the \u001b[1m--model \u001b[22mand \u001b[1m--image\u001b[0m\n",
      "               flags should also be supplied.\n",
      "\n",
      "       \u001b[1m-s \u001b[4m\u001b[22mSEED\u001b[24m, \u001b[1m--seed \u001b[4m\u001b[22mSEED\u001b[0m\n",
      "               Random Number Generator (RNG) seed. A random seed  is  used  if\n",
      "               this is less than zero.\n",
      "\n",
      "               Default: -1\n",
      "\n",
      "       \u001b[1m-t \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--threads \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of threads to use during generation.\n",
      "\n",
      "               Default: $(nproc)/2\n",
      "\n",
      "       \u001b[1m-tb \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--threads-batch \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Set  the  number of threads to use during batch and prompt pro‐\n",
      "               cessing. In some systems, it is beneficial to use a higher num‐\n",
      "               ber of threads during batch processing than during  generation.\n",
      "               If not specified, the number of threads used for batch process‐\n",
      "               ing  will be the same as the number of threads used for genera‐\n",
      "               tion.\n",
      "\n",
      "               Default: Same as \u001b[1m--threads\u001b[0m\n",
      "\n",
      "       \u001b[1m-td \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--threads-draft \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of threads to use during generation.\n",
      "\n",
      "               Default: Same as \u001b[1m--threads\u001b[0m\n",
      "\n",
      "       \u001b[1m-tbd \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--threads-batch-draft \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of threads to use during batch and prompt processing.\n",
      "\n",
      "               Default: Same as \u001b[1m--threads-draft\u001b[0m\n",
      "\n",
      "       \u001b[1m--in-prefix-bos\u001b[0m\n",
      "               Prefix BOS to user inputs, preceding the \u001b[1m--in-prefix \u001b[22mstring.\n",
      "\n",
      "       \u001b[1m--in-prefix \u001b[4m\u001b[22mSTRING\u001b[0m\n",
      "               This flag is used to add a prefix  to  your  input,  primarily,\n",
      "               this is used to insert a space after the reverse prompt. Here's\n",
      "               an  example  of  how to use the \u001b[1m--in-prefix \u001b[22mflag in conjunction\n",
      "               with the \u001b[1m--reverse-prompt \u001b[22mflag:\n",
      "\n",
      "                     \u001b[1m./main -r \"User:\" --in-prefix \" \"\u001b[0m\n",
      "\n",
      "               Default: empty\n",
      "\n",
      "       \u001b[1m--in-suffix \u001b[4m\u001b[22mSTRING\u001b[0m\n",
      "               This flag is used to add a suffix after  your  input.  This  is\n",
      "               useful  for  adding an \"Assistant:\" prompt after the user's in‐\n",
      "               put. It's added after the new-line character (\\n) that's  auto‐\n",
      "               matically added to the end of the user's input. Here's an exam‐\n",
      "               ple  of how to use the \u001b[1m--in-suffix \u001b[22mflag in conjunction with the\n",
      "               \u001b[1m--reverse-prompt \u001b[22mflag:\n",
      "\n",
      "                     \u001b[1m./main   -r   \"User:\"   --in-prefix   \"   \"   --in-suffix\u001b[0m\n",
      "                     \u001b[1m\"Assistant:\"\u001b[0m\n",
      "\n",
      "               Default: empty\n",
      "\n",
      "       \u001b[1m-n \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--n-predict \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of tokens to predict.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m-1 = infinity\n",
      "               \u001b[1m-   \u001b[22m-2 = until context filled\n",
      "\n",
      "               Default: -1\n",
      "\n",
      "       \u001b[1m-c \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--ctx-size \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Set the size of the prompt context. A larger context size helps\n",
      "               the  model  to  better  comprehend  and  generate responses for\n",
      "               longer input or conversations. The LLaMA models were built with\n",
      "               a context of 2048, which yields the best results on longer  in‐\n",
      "               put / inference.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0 = loaded automatically from model\n",
      "\n",
      "               Default: 512\n",
      "\n",
      "       \u001b[1m-b \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--batch-size \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Batch size for prompt processing.\n",
      "\n",
      "               Default: 512\n",
      "\n",
      "       \u001b[1m--top-k \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Top-k sampling.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0 = disabled\n",
      "\n",
      "               Default: 40\n",
      "\n",
      "       \u001b[1m--top-p \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Top-p sampling.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m1.0 = disabled\n",
      "\n",
      "               Default: 0.9\n",
      "\n",
      "       \u001b[1m--min-p \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Min-p sampling.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0.0 = disabled\n",
      "\n",
      "               Default: 0.1\n",
      "\n",
      "       \u001b[1m--tfs \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Tail free sampling, parameter z.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m1.0 = disabled\n",
      "\n",
      "               Default: 1.0\n",
      "\n",
      "       \u001b[1m--typical \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Locally typical sampling, parameter p.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m1.0 = disabled\n",
      "\n",
      "               Default: 1.0\n",
      "\n",
      "       \u001b[1m--repeat-last-n \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Last n tokens to consider for penalize.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0 = disabled\n",
      "               \u001b[1m-   \u001b[22m-1 = ctx_size\n",
      "\n",
      "               Default: 64\n",
      "\n",
      "       \u001b[1m--repeat-penalty \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Penalize repeat sequence of tokens.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m1.0 = disabled\n",
      "\n",
      "               Default: 1.1\n",
      "\n",
      "       \u001b[1m--presence-penalty \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Repeat alpha presence penalty.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0.0 = disabled\n",
      "\n",
      "               Default: 0.0\n",
      "\n",
      "       \u001b[1m--frequency-penalty \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Repeat alpha frequency penalty.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0.0 = disabled\n",
      "\n",
      "               Default: 0.0\n",
      "\n",
      "       \u001b[1m--mirostat \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Use  Mirostat  sampling.  Top K, Nucleus, Tail Free and Locally\n",
      "               Typical samplers are ignored if used..\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0 = disabled\n",
      "               \u001b[1m-   \u001b[22m1 = Mirostat\n",
      "               \u001b[1m-   \u001b[22m2 = Mirostat 2.0\n",
      "\n",
      "               Default: 0\n",
      "\n",
      "       \u001b[1m--mirostat-lr \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Mirostat learning rate, parameter eta.\n",
      "\n",
      "               Default: 0.1\n",
      "\n",
      "       \u001b[1m--mirostat-ent \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Mirostat target entropy, parameter tau.\n",
      "\n",
      "               Default: 5.0\n",
      "\n",
      "       \u001b[1m-l \u001b[4m\u001b[22mTOKEN_ID(+/-)BIAS\u001b[24m, \u001b[1m--logit-bias \u001b[4m\u001b[22mTOKEN_ID(+/-)BIAS\u001b[0m\n",
      "               Modifies the likelihood of token appearing in  the  completion,\n",
      "               i.e.   \u001b[1m--logit-bias  \u001b[4m\u001b[22m15043+1\u001b[24m  to  increase  likelihood of token\n",
      "               \u001b[4m'\u001b[24m \u001b[4mHello'\u001b[24m, or \u001b[1m--logit-bias \u001b[4m\u001b[22m15043-1\u001b[24m to decrease likelihood of to‐\n",
      "               ken \u001b[4m'\u001b[24m \u001b[4mHello'\u001b[24m.\n",
      "\n",
      "       \u001b[1m-md \u001b[4m\u001b[22mFNAME\u001b[24m, \u001b[1m--model-draft \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Draft model for speculative decoding.\n",
      "\n",
      "               Default: \u001b[4mmodels/7B/ggml-model-f16.gguf\u001b[0m\n",
      "\n",
      "       \u001b[1m--cfg-negative-prompt \u001b[4m\u001b[22mPROMPT\u001b[0m\n",
      "               Negative prompt to use for guidance..\n",
      "\n",
      "               Default: empty\n",
      "\n",
      "       \u001b[1m--cfg-negative-prompt-file \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Negative prompt file to use for guidance.\n",
      "\n",
      "               Default: empty\n",
      "\n",
      "       \u001b[1m--cfg-scale \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Strength of guidance.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m1.0 = disable\n",
      "\n",
      "               Default: 1.0\n",
      "\n",
      "       \u001b[1m--rope-scaling \u001b[4m\u001b[22m{none,linear,yarn}\u001b[0m\n",
      "               RoPE frequency scaling method, defaults to linear unless speci‐\n",
      "               fied by the model\n",
      "\n",
      "       \u001b[1m--rope-scale \u001b[4m\u001b[22mN\u001b[0m\n",
      "               RoPE context scaling factor, expands context by a factor  of  \u001b[4mN\u001b[0m\n",
      "               where  \u001b[4mN\u001b[24m  is  the  linear scaling factor used by the fine-tuned\n",
      "               model. Some fine-tuned models have extended the context  length\n",
      "               by scaling RoPE. For example, if the original pre-trained model\n",
      "               have  a  context  length (max sequence length) of 4096 (4k) and\n",
      "               the fine-tuned model have 32k. That is a scaling factor  of  8,\n",
      "               and  should work by setting the above \u001b[1m--ctx-size \u001b[22mto 32768 (32k)\n",
      "               and \u001b[1m--rope-scale \u001b[22mto 8.\n",
      "\n",
      "       \u001b[1m--rope-freq-base \u001b[4m\u001b[22mN\u001b[0m\n",
      "               RoPE base frequency, used by NTK-aware scaling.\n",
      "\n",
      "               Default: loaded from model\n",
      "\n",
      "       \u001b[1m--rope-freq-scale \u001b[4m\u001b[22mN\u001b[0m\n",
      "               RoPE frequency scaling factor, expands context by a  factor  of\n",
      "               1/N\n",
      "\n",
      "       \u001b[1m--yarn-orig-ctx \u001b[4m\u001b[22mN\u001b[0m\n",
      "               YaRN: original context size of model.\n",
      "\n",
      "               Default: 0 = model training context size\n",
      "\n",
      "       \u001b[1m--yarn-ext-factor \u001b[4m\u001b[22mN\u001b[0m\n",
      "               YaRN: extrapolation mix factor.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0.0 = full interpolation\n",
      "\n",
      "               Default: 1.0\n",
      "\n",
      "       \u001b[1m--yarn-attn-factor \u001b[4m\u001b[22mN\u001b[0m\n",
      "               YaRN: scale sqrt(t) or attention magnitude.\n",
      "\n",
      "               Default: 1.0\n",
      "\n",
      "       \u001b[1m--yarn-beta-slow \u001b[4m\u001b[22mN\u001b[0m\n",
      "               YaRN: high correction dim or alpha.\n",
      "\n",
      "               Default: 1.0\n",
      "\n",
      "       \u001b[1m--yarn-beta-fast \u001b[4m\u001b[22mN\u001b[0m\n",
      "               YaRN: low correction dim or beta.\n",
      "\n",
      "               Default: 32.0\n",
      "\n",
      "       \u001b[1m--ignore-eos\u001b[0m\n",
      "               Ignore  end  of  stream  token and continue generating (implies\n",
      "               \u001b[1m--logit-bias \u001b[4m\u001b[22m2-inf\u001b[24m)\n",
      "\n",
      "       \u001b[1m--no-penalize-nl\u001b[0m\n",
      "               Do not penalize newline token.\n",
      "\n",
      "       \u001b[1m--temp \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Temperature.\n",
      "\n",
      "               Default: 0.8\n",
      "\n",
      "       \u001b[1m--logits-all\u001b[0m\n",
      "               Return logits for all tokens in the batch.\n",
      "\n",
      "               Default: disabled\n",
      "\n",
      "       \u001b[1m--hellaswag\u001b[0m\n",
      "               Compute HellaSwag score over random tasks  from  datafile  sup‐\n",
      "               plied with -f\n",
      "\n",
      "       \u001b[1m--hellaswag-tasks \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of tasks to use when computing the HellaSwag score.\n",
      "\n",
      "               Default: 400\n",
      "\n",
      "       \u001b[1m--keep \u001b[4m\u001b[22mN\u001b[0m\n",
      "               This  flag  allows users to retain the original prompt when the\n",
      "               model runs out of context, ensuring a connection to the initial\n",
      "               instruction or conversation topic is maintained, where \u001b[4mN\u001b[24m is the\n",
      "               number of tokens from the initial prompt  to  retain  when  the\n",
      "               model resets its internal context.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m0 = no tokens are kept from initial prompt\n",
      "               \u001b[1m-   \u001b[22m-1 = retain all tokens from initial prompt\n",
      "\n",
      "               Default: 0\n",
      "\n",
      "       \u001b[1m--draft \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of tokens to draft for speculative decoding.\n",
      "\n",
      "               Default: 16\n",
      "\n",
      "       \u001b[1m--chunks \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Max number of chunks to process.\n",
      "\n",
      "               \u001b[1m-   \u001b[22m-1 = all\n",
      "\n",
      "               Default: -1\n",
      "\n",
      "       \u001b[1m-ns \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--sequences \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of sequences to decode.\n",
      "\n",
      "               Default: 1\n",
      "\n",
      "       \u001b[1m-pa \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--p-accept \u001b[4m\u001b[22mN\u001b[0m\n",
      "               speculative decoding accept probability.\n",
      "\n",
      "               Default: 0.5\n",
      "\n",
      "       \u001b[1m-ps \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--p-split \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Speculative decoding split probability.\n",
      "\n",
      "               Default: 0.1\n",
      "\n",
      "       \u001b[1m--mlock\u001b[0m\n",
      "               Force  system to keep model in RAM rather than swapping or com‐\n",
      "               pressing.\n",
      "\n",
      "       \u001b[1m--no-mmap\u001b[0m\n",
      "               Do not memory-map model (slower load but may reduce pageouts if\n",
      "               not using mlock).\n",
      "\n",
      "       \u001b[1m--numa  \u001b[22mAttempt optimizations that help on some  NUMA  systems  if  run\n",
      "               without  this  previously, it is recommended to drop the system\n",
      "               page       cache       before       using       this.       See\n",
      "               https://github.com/ggerganov/llama.cpp/issues/1437.\n",
      "\n",
      "       \u001b[1m--recompile\u001b[0m\n",
      "               Force GPU support to be recompiled at runtime if possible.\n",
      "\n",
      "       \u001b[1m--nocompile\u001b[0m\n",
      "               Never compile GPU support at runtime.\n",
      "\n",
      "               If  the appropriate DSO file already exists under \u001b[4m~/.llamafile/\u001b[0m\n",
      "               then it'll be linked as-is without question. If a prebuilt  DSO\n",
      "               is  present  in the PKZIP content of the executable, then it'll\n",
      "               be extracted and linked if possible. Otherwise, \u001b[1mllamafile  \u001b[22mwill\n",
      "               skip any attempt to compile GPU support and simply fall back to\n",
      "               using CPU inference.\n",
      "\n",
      "       \u001b[1m--gpu \u001b[4m\u001b[22mGPU\u001b[0m\n",
      "               Specifies which brand of GPU should be used. Valid choices are:\n",
      "\n",
      "               \u001b[1m-   \u001b[4m\u001b[22mAUTO\u001b[24m:  Use  any GPU if possible, otherwise fall back to CPU\n",
      "                   inference (default)\n",
      "\n",
      "               \u001b[1m-   \u001b[4m\u001b[22mAPPLE\u001b[24m: Use Apple Metal GPU. This is only available on MacOS\n",
      "                   ARM64. If Metal could not be used for any  reason,  then  a\n",
      "                   fatal error will be raised.\n",
      "\n",
      "               \u001b[1m-   \u001b[4m\u001b[22mAMD\u001b[24m: Use AMD GPUs. The AMD HIP ROCm SDK should be installed\n",
      "                   in  which  case we assume the HIP_PATH environment variable\n",
      "                   has been defined. The set of gfx microarchitectures  needed\n",
      "                   to  run  on  the  host  machine is determined automatically\n",
      "                   based on the output of the  hipInfo  command.  On  Windows,\n",
      "                   \u001b[1mllamafile  \u001b[22mrelease binaries are distributed with a tinyBLAS\n",
      "                   DLL so it'll work out of the box without requiring the  HIP\n",
      "                   SDK  to  be  installed.  However,  tinyBLAS  is slower than\n",
      "                   rocBLAS for batch and image processing, so it's recommended\n",
      "                   that the SDK be installed anyway. If an AMD GPU  could  not\n",
      "                   be used for any reason, then a fatal error will be raised.\n",
      "\n",
      "               \u001b[1m-   \u001b[4m\u001b[22mNVIDIA\u001b[24m: Use NVIDIA GPUs. If an NVIDIA GPU could not be used\n",
      "                   for  any  reason, a fatal error will be raised. On Windows,\n",
      "                   NVIDIA GPU support will use our tinyBLAS library, since  it\n",
      "                   works  on  stock  Windows  installs. However, tinyBLAS goes\n",
      "                   slower for batch and image processing. It's possible to use\n",
      "                   NVIDIA's closed-source cuBLAS library instead. To do  that,\n",
      "                   both  MSVC  and CUDA need to be installed and the \u001b[1mllamafile\u001b[0m\n",
      "                   command should be run once from the x64 MSVC command prompt\n",
      "                   with the \u001b[1m--recompile \u001b[22mflag passed.  The  GGML  library  will\n",
      "                   then  be compiled and saved to \u001b[4m~/.llamafile/\u001b[24m so the special\n",
      "                   process only needs to happen a single time.\n",
      "\n",
      "               \u001b[1m-   \u001b[4m\u001b[22mDISABLE\u001b[24m: Never use GPU and instead use CPU inference.  This\n",
      "                   setting is implied by \u001b[1m-ngl \u001b[4m\u001b[22m0\u001b[24m.\n",
      "\n",
      "       \u001b[1m-ngl \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--n-gpu-layers \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of layers to store in VRAM.\n",
      "\n",
      "       \u001b[1m-ngld \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--n-gpu-layers-draft \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of layers to store in VRAM for the draft model.\n",
      "\n",
      "       \u001b[1m-sm \u001b[4m\u001b[22mSPLIT_MODE\u001b[24m, \u001b[1m--split-mode \u001b[4m\u001b[22mSPLIT_MODE\u001b[0m\n",
      "               How to split the model across multiple GPUs, one of:\n",
      "               \u001b[1m-   \u001b[22mnone: use one GPU only\n",
      "               \u001b[1m-   \u001b[22mlayer (default): split layers and KV across GPUs\n",
      "               \u001b[1m-   \u001b[22mrow: split rows across GPUs\n",
      "\n",
      "       \u001b[1m-ts \u001b[4m\u001b[22mSPLIT\u001b[24m, \u001b[1m--tensor-split \u001b[4m\u001b[22mSPLIT\u001b[0m\n",
      "               When using multiple GPUs this option controls how large tensors\n",
      "               should  be  split  across all GPUs.  \u001b[4mSPLIT\u001b[24m is a comma-separated\n",
      "               list of non-negative values that assigns the proportion of data\n",
      "               that each GPU should get in order. For example, \"3,2\" will  as‐\n",
      "               sign  60% of the data to GPU 0 and 40% to GPU 1. By default the\n",
      "               data is split in proportion to VRAM but this may not be optimal\n",
      "               for performance. Requires cuBLAS.  How to split tensors  across\n",
      "               multiple GPUs, comma-separated list of proportions, e.g. 3,1\n",
      "\n",
      "       \u001b[1m-mg \u001b[4m\u001b[22mi\u001b[24m, \u001b[1m--main-gpu \u001b[4m\u001b[22mi\u001b[0m\n",
      "               The GPU to use for scratch and small tensors.\n",
      "\n",
      "       \u001b[1m-nommq\u001b[22m, \u001b[1m--no-mul-mat-q\u001b[0m\n",
      "               Use cuBLAS instead of custom mul_mat_q CUDA kernels. Not recom‐\n",
      "               mended since this is both slower and uses more VRAM.\n",
      "\n",
      "       \u001b[1m--verbose-prompt\u001b[0m\n",
      "               Print prompt before generation.\n",
      "\n",
      "       \u001b[1m--simple-io\u001b[0m\n",
      "               Use  basic IO for better compatibility in subprocesses and lim‐\n",
      "               ited consoles.\n",
      "\n",
      "       \u001b[1m--lora \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Apply LoRA adapter (implies \u001b[1m--no-mmap\u001b[22m)\n",
      "\n",
      "       \u001b[1m--lora-scaled \u001b[4m\u001b[22mFNAME\u001b[24m \u001b[4mS\u001b[0m\n",
      "               Apply  LoRA  adapter  with  user  defined  scaling  S  (implies\n",
      "               \u001b[1m--no-mmap\u001b[22m)\n",
      "\n",
      "       \u001b[1m--lora-base \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Optional  model to use as a base for the layers modified by the\n",
      "               LoRA adapter\n",
      "\n",
      "       \u001b[1m--unsecure\u001b[0m\n",
      "               Disables pledge() sandboxing on Linux and OpenBSD.\n",
      "\n",
      "       \u001b[1m--samplers\u001b[0m\n",
      "               Samplers that will be used for generation in the  order,  sepa‐\n",
      "               rated    by    semicolon,    for    example:    top_k;tfs;typi‐\n",
      "               cal;top_p;min_p;temp\n",
      "\n",
      "       \u001b[1m--samplers-seq\u001b[0m\n",
      "               Simplified sequence for samplers that will be used.\n",
      "\n",
      "       \u001b[1m-cml\u001b[22m, \u001b[1m--chatml\u001b[0m\n",
      "               Run in chatml mode (use with ChatML-compatible models)\n",
      "\n",
      "       \u001b[1m-dkvc\u001b[22m, \u001b[1m--dump-kv-cache\u001b[0m\n",
      "               Verbose print of the KV cache.\n",
      "\n",
      "       \u001b[1m-nkvo\u001b[22m, \u001b[1m--no-kv-offload\u001b[0m\n",
      "               Disable KV offload.\n",
      "\n",
      "       \u001b[1m-ctk \u001b[4m\u001b[22mTYPE\u001b[24m, \u001b[1m--cache-type-k \u001b[4m\u001b[22mTYPE\u001b[0m\n",
      "               KV cache data type for K.\n",
      "\n",
      "       \u001b[1m-ctv \u001b[4m\u001b[22mTYPE\u001b[24m, \u001b[1m--cache-type-v \u001b[4m\u001b[22mTYPE\u001b[0m\n",
      "               KV cache data type for V.\n",
      "\n",
      "       \u001b[1m-gan \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--grp-attn-n \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Group-attention factor.\n",
      "\n",
      "               Default: 1\n",
      "\n",
      "       \u001b[1m-gaw \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--grp-attn-w \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Group-attention width.\n",
      "\n",
      "               Default: 512\n",
      "\n",
      "       \u001b[1m-bf \u001b[4m\u001b[22mFNAME\u001b[24m, \u001b[1m--binary-file \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Binary file containing multiple choice tasks.\n",
      "\n",
      "       \u001b[1m--winogrande\u001b[0m\n",
      "               Compute Winogrande score over random tasks from  datafile  sup‐\n",
      "               plied by the \u001b[1m-f \u001b[22mflag.\n",
      "\n",
      "       \u001b[1m--winogrande-tasks \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of tasks to use when computing the Winogrande score.\n",
      "\n",
      "               Default: 0\n",
      "\n",
      "       \u001b[1m--multiple-choice\u001b[0m\n",
      "               Compute  multiple  choice score over random tasks from datafile\n",
      "               supplied by the \u001b[1m-f \u001b[22mflag.\n",
      "\n",
      "       \u001b[1m--multiple-choice-tasks \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of tasks to  use  when  computing  the  multiple  choice\n",
      "               score.\n",
      "\n",
      "               Default: 0\n",
      "\n",
      "       \u001b[1m--kl-divergence\u001b[0m\n",
      "               Computes    KL-divergence    to   logits   provided   via   the\n",
      "               \u001b[1m--kl-divergence-base \u001b[22mflag.\n",
      "\n",
      "       \u001b[1m--save-all-logits \u001b[4m\u001b[22mFNAME\u001b[24m, \u001b[1m--kl-divergence-base \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Save logits to filename.\n",
      "\n",
      "       \u001b[1m-ptc \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--print-token-count \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Print token count every \u001b[4mN\u001b[24m tokens.\n",
      "\n",
      "               Default: -1\n",
      "\n",
      "       \u001b[1m--pooling \u001b[4m\u001b[22mKIND\u001b[0m\n",
      "               Specifies pooling type for embeddings. This may be one of:\n",
      "\n",
      "               \u001b[1m-   \u001b[22mnone\n",
      "               \u001b[1m-   \u001b[22mmean\n",
      "               \u001b[1m-   \u001b[22mcls\n",
      "\n",
      "               The model default is used if unspecified.\n",
      "\n",
      "\u001b[1mCLI OPTIONS\u001b[0m\n",
      "       The following options may be specified when  \u001b[1mllamafile  \u001b[22mis  running  in\n",
      "       \u001b[1m--cli \u001b[22mmode.\n",
      "\n",
      "       \u001b[1m-e\u001b[22m, \u001b[1m--escape\u001b[0m\n",
      "               Process prompt escapes sequences (\\n, \\r, \\t, \\´, \\\", \\\\)\n",
      "\n",
      "       \u001b[1m-p \u001b[4m\u001b[22mSTRING\u001b[24m, \u001b[1m--prompt \u001b[4m\u001b[22mSTRING\u001b[0m\n",
      "               Prompt  to  start  text generation. Your LLM works by auto-com‐\n",
      "               pleting this text. For example:\n",
      "\n",
      "                     \u001b[1mllamafile -m model.gguf -p \"four score and\"\u001b[0m\n",
      "\n",
      "               Stands a pretty good chance of  printing  Lincoln's  Gettysburg\n",
      "               Address.   Prompts can take on a structured format too. Depend‐\n",
      "               ing on how your model was trained, it may specify in  its  docs\n",
      "               an instruction notation. With some models that might be:\n",
      "\n",
      "                     \u001b[1mllamafile -p \"[INST]Summarize this: $(cat file)[/INST]\"\u001b[0m\n",
      "\n",
      "               In most cases, simply colons and newlines will work too:\n",
      "\n",
      "                     \u001b[1mllamafile -e -p \"User: What is best in life?\\nAssistant:\"\u001b[0m\n",
      "\n",
      "       \u001b[1m-f \u001b[4m\u001b[22mFNAME\u001b[24m, \u001b[1m--file \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Prompt file to start generation.\n",
      "\n",
      "       \u001b[1m--grammar \u001b[4m\u001b[22mGRAMMAR\u001b[0m\n",
      "               BNF-like grammar to constrain which tokens may be selected when\n",
      "               generating text. For example, the grammar:\n",
      "\n",
      "                     \u001b[1mroot ::= \"yes\" | \"no\"\u001b[0m\n",
      "\n",
      "               will  force  the  LLM  to only output yes or no before exiting.\n",
      "               This is useful for shell scripts when  the  \u001b[1m--no-display-prompt\u001b[0m\n",
      "               flag is also supplied.\n",
      "\n",
      "       \u001b[1m--grammar-file \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               File to read grammar from.\n",
      "\n",
      "       \u001b[1m--fast  \u001b[22mPut  llamafile  into  fast  math mode. This disables algorithms\n",
      "               that reduce floating point rounding, e.g. Kahan summation,  and\n",
      "               certain functions like expf() will be vectorized but handle un‐\n",
      "               derflows  less  gracefully.  It's unspecified whether llamafile\n",
      "               runs in fast or precise math mode when neither flag  is  speci‐\n",
      "               fied.\n",
      "\n",
      "       \u001b[1m--precise\u001b[0m\n",
      "               Put  llamafile  into precise math mode. This enables algorithms\n",
      "               that reduce floating point rounding, e.g. Kahan summation,  and\n",
      "               certain  functions  like  expf()  will always handle subnormals\n",
      "               correctly. It's unspecified whether llamafile runs in  fast  or\n",
      "               precise math mode when neither flag is specified.\n",
      "\n",
      "       \u001b[1m--trap  \u001b[22mPut  llamafile into math trapping mode. When floating point ex‐\n",
      "               ceptions occur, such as NaNs, overflow,  and  divide  by  zero,\n",
      "               llamafile  will  print  a  warning to the console. This warning\n",
      "               will include a C++ backtrace the first  time  an  exception  is\n",
      "               trapped.  The  op graph will also be dumped to a file, and lla‐\n",
      "               mafile will report the specific  op  where  the  exception  oc‐\n",
      "               curred.  This  is useful for troubleshooting when reporting is‐\n",
      "               sues.  USing this feature will disable sandboxing.  Math  trap‐\n",
      "               ping  is  only possible if your CPU supports it. That is gener‐\n",
      "               ally the case on AMD64, however it's less common on ARM64.\n",
      "\n",
      "       \u001b[1m--prompt-cache \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               File to cache prompt state for faster startup.\n",
      "\n",
      "               Default: none\n",
      "\n",
      "       \u001b[1m-fa \u001b[4m\u001b[22mFNAME\u001b[24m, \u001b[1m--flash-attn\u001b[0m\n",
      "               Enable Flash Attention. This is a  mathematical  shortcut  that\n",
      "               can  speed  up  inference  for  certain models. This feature is\n",
      "               still under active development.\n",
      "\n",
      "       \u001b[1m--prompt-cache-all\u001b[0m\n",
      "               If specified, saves user input  and  generations  to  cache  as\n",
      "               well. Not supported with \u001b[1m--interactive \u001b[22mor other interactive op‐\n",
      "               tions.\n",
      "\n",
      "       \u001b[1m--prompt-cache-ro\u001b[0m\n",
      "               If specified, uses the prompt cache but does not update it.\n",
      "\n",
      "       \u001b[1m--random-prompt\u001b[0m\n",
      "               Start with a randomized prompt.\n",
      "\n",
      "       \u001b[1m--image \u001b[4m\u001b[22mIMAGE_FILE\u001b[0m\n",
      "               Path to an image file. This should be used with multimodal mod‐\n",
      "               els.   Alternatively,  it's possible to embed an image directly\n",
      "               into the prompt instead; in which case, it must be  base64  en‐\n",
      "               coded  into  an HTML img tag URL with the image/jpeg MIME type.\n",
      "               See also the \u001b[1m--mmproj \u001b[22mflag for supplying the vision model.\n",
      "\n",
      "       \u001b[1m-i\u001b[22m, \u001b[1m--interactive\u001b[0m\n",
      "               Run the program in interactive mode, allowing users  to  engage\n",
      "               in  real-time conversations or provide specific instructions to\n",
      "               the model.\n",
      "\n",
      "       \u001b[1m--interactive-first\u001b[0m\n",
      "               Run the program in interactive mode and  immediately  wait  for\n",
      "               user input before starting the text generation.\n",
      "\n",
      "       \u001b[1m-ins\u001b[22m, \u001b[1m--instruct\u001b[0m\n",
      "               Run  the program in instruction mode, which is specifically de‐\n",
      "               signed to work with Alpaca  models  that  excel  in  completing\n",
      "               tasks based on user instructions.\n",
      "\n",
      "               Technical details: The user's input is internally prefixed with\n",
      "               the  reverse prompt (or \"### Instruction:\" as the default), and\n",
      "               followed by \"### Response:\" (except if you  just  press  Return\n",
      "               without any input, to keep generating a longer response).\n",
      "\n",
      "               By  understanding  and utilizing these interaction options, you\n",
      "               can create engaging and dynamic experiences with the LLaMA mod‐\n",
      "               els, tailoring the text generation  process  to  your  specific\n",
      "               needs.\n",
      "\n",
      "       \u001b[1m-r \u001b[4m\u001b[22mPROMPT\u001b[24m, \u001b[1m--reverse-prompt \u001b[4m\u001b[22mPROMPT\u001b[0m\n",
      "               Specify  one  or multiple reverse prompts to pause text genera‐\n",
      "               tion and switch to interactive mode. For  example,  \u001b[1m-r  \u001b[4m\u001b[22m\"User:\"\u001b[0m\n",
      "               can  be  used  to jump back into the conversation whenever it's\n",
      "               the user's turn to speak. This helps create a more  interactive\n",
      "               and  conversational  experience.  However,  the  reverse prompt\n",
      "               doesn't work when it ends with a space. To overcome this  limi‐\n",
      "               tation,  you can use the \u001b[1m--in-prefix \u001b[22mflag to add a space or any\n",
      "               other characters after the reverse prompt.\n",
      "\n",
      "       \u001b[1m--color\u001b[0m\n",
      "               Enable colorized output to differentiate visually  distinguish‐\n",
      "               ing between prompts, user input, and generated text.\n",
      "\n",
      "       \u001b[1m--no-display-prompt\u001b[22m, \u001b[1m--silent-prompt\u001b[0m\n",
      "               Don't echo the prompt itself to standard output.\n",
      "\n",
      "       \u001b[1m--keep \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Specifies number of tokens to keep from the initial prompt. The\n",
      "               default is -1 which means all tokens.\n",
      "\n",
      "       \u001b[1m--multiline-input\u001b[0m\n",
      "               Allows you to write or paste multiple lines without ending each\n",
      "               in '\\'.\n",
      "\n",
      "       \u001b[1m--cont-batching\u001b[0m\n",
      "               Enables  continuous  batching,  a.k.a. dynamic batching.  is -1\n",
      "               which means all tokens.\n",
      "\n",
      "       \u001b[1m--embedding\u001b[0m\n",
      "               In CLI mode, the embedding flag may be use to print  embeddings\n",
      "               to  standard output. By default, embeddings are computed over a\n",
      "               whole prompt. However the \u001b[1m--multiline \u001b[22mflag may  be  passed,  to\n",
      "               have a separate embeddings array computed for each line of text\n",
      "               in  the prompt. In multiline mode, each embedding array will be\n",
      "               printed on its own line to standard  output,  where  individual\n",
      "               floats  are  separated  by space. If both the \u001b[1m--multiline-input\u001b[0m\n",
      "               and \u001b[1m--interactive \u001b[22mflags are passed, then a pretty-printed  sum‐\n",
      "               mary  of  embeddings along with a cosine similarity matrix will\n",
      "               be printed to the terminal.\n",
      "\n",
      "\u001b[1mSERVER OPTIONS\u001b[0m\n",
      "       The following options may be specified when  \u001b[1mllamafile  \u001b[22mis  running  in\n",
      "       \u001b[1m--server \u001b[22mmode.\n",
      "\n",
      "       \u001b[1m--port \u001b[4m\u001b[22mPORT\u001b[0m\n",
      "               Port to listen\n",
      "\n",
      "               Default: 8080\n",
      "\n",
      "       \u001b[1m--host \u001b[4m\u001b[22mIPADDR\u001b[0m\n",
      "               IP address to listen.\n",
      "\n",
      "               Default: 127.0.0.1\n",
      "\n",
      "       \u001b[1m-to \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--timeout \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Server read/write timeout in seconds.\n",
      "\n",
      "               Default: 600\n",
      "\n",
      "       \u001b[1m-np \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--parallel \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Number of slots for process requests.\n",
      "\n",
      "               Default: 1\n",
      "\n",
      "       \u001b[1m-cb\u001b[22m, \u001b[1m--cont-batching\u001b[0m\n",
      "               Enable continuous batching (a.k.a dynamic batching).\n",
      "\n",
      "               Default: disabled\n",
      "\n",
      "       \u001b[1m-spf \u001b[4m\u001b[22mFNAME\u001b[24m, \u001b[1m--system-prompt-file \u001b[4m\u001b[22mFNAME\u001b[0m\n",
      "               Set  a  file  to  load  a  system prompt (initial prompt of all\n",
      "               slots), this is useful for chat applications.\n",
      "\n",
      "       \u001b[1m-a \u001b[4m\u001b[22mALIAS\u001b[24m, \u001b[1m--alias \u001b[4m\u001b[22mALIAS\u001b[0m\n",
      "               Set an alias for the model. This will be  added  as  the  \u001b[4mmodel\u001b[0m\n",
      "               field in completion responses.\n",
      "\n",
      "       \u001b[1m--path \u001b[4m\u001b[22mPUBLIC_PATH\u001b[0m\n",
      "               Path from which to serve static files.\n",
      "\n",
      "               Default: \u001b[4m/zip/llama.cpp/server/public\u001b[0m\n",
      "\n",
      "       \u001b[1m--nobrowser\u001b[0m\n",
      "               Do not attempt to open a web browser tab at startup.\n",
      "\n",
      "       \u001b[1m-gan \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--grp-attn-n \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Set  the  group attention factor to extend context size through\n",
      "               self-extend. The default value is \u001b[4m1\u001b[24m which means disabled.  This\n",
      "               flag is used together with \u001b[1m--grp-attn-w\u001b[22m.\n",
      "\n",
      "       \u001b[1m-gaw \u001b[4m\u001b[22mN\u001b[24m, \u001b[1m--grp-attn-w \u001b[4m\u001b[22mN\u001b[0m\n",
      "               Set  the  group  attention width to extend context size through\n",
      "               self-extend. The default value is \u001b[4m512\u001b[24m.  This flag is  used  to‐\n",
      "               gether with \u001b[1m--grp-attn-n\u001b[22m.\n",
      "\n",
      "\u001b[1mLOG OPTIONS\u001b[0m\n",
      "       The following log options are available:\n",
      "\n",
      "       \u001b[1m-ld \u001b[4m\u001b[22mLOGDIR\u001b[24m, \u001b[1m--logdir \u001b[4m\u001b[22mLOGDIR\u001b[0m\n",
      "               Path under which to save YAML logs (no logging if unset)\n",
      "\n",
      "       \u001b[1m--log-test\u001b[0m\n",
      "               Run simple logging test\n",
      "\n",
      "       \u001b[1m--log-disable\u001b[0m\n",
      "               Disable trace logs\n",
      "\n",
      "       \u001b[1m--log-enable\u001b[0m\n",
      "               Enable trace logs\n",
      "\n",
      "       \u001b[1m--log-file\u001b[0m\n",
      "               Specify a log filename (without extension)\n",
      "\n",
      "       \u001b[1m--log-new\u001b[0m\n",
      "               Create  a  separate  new  log file on start. Each log file will\n",
      "               have unique name: \u001b[4m<name>.<ID>.log\u001b[0m\n",
      "\n",
      "       \u001b[1m--log-append\u001b[0m\n",
      "               Don't truncate the old log file.\n",
      "\n",
      "\u001b[1mEXAMPLES\u001b[0m\n",
      "       Here's an example of how to run llama.cpp's built-in HTTP server.  This\n",
      "       example   uses   LLaVA  v1.5-7B,  a  multimodal  LLM  that  works  with\n",
      "       llama.cpp's recently-added support for image inputs.\n",
      "\n",
      "             llamafile \\\n",
      "               -m llava-v1.5-7b-Q8_0.gguf \\\n",
      "               --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \\\n",
      "               --host 0.0.0.0\n",
      "\n",
      "       Here's an example of how to generate code for a libc function using the\n",
      "       llama.cpp  command  line  interface,  utilizing  WizardCoder-Python-13B\n",
      "       weights:\n",
      "\n",
      "             llamafile \\\n",
      "               -m wizardcoder-python-13b-v1.0.Q8_0.gguf --temp 0 -r '}\\n' -r '```\\n' \\\n",
      "               -e -p '```c\\nvoid *memcpy(void *dst, const void *src, size_t size) {\\n'\n",
      "\n",
      "       Here's  a  similar  example  that  instead utilizes Mistral-7B-Instruct\n",
      "       weights for prose composition:\n",
      "\n",
      "             llamafile \\\n",
      "               -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \\\n",
      "               -p '[INST]Write a story about llamas[/INST]'\n",
      "\n",
      "       Here's an example of how llamafile can be used as an interactive  chat‐\n",
      "       bot that lets you query knowledge contained in training data:\n",
      "\n",
      "             llamafile -m llama-65b-Q5_K.gguf -p '\n",
      "             The following is a conversation between a Researcher and their helpful AI\n",
      "             assistant Digital Athena which is a large language model trained on the\n",
      "             sum of human knowledge.\n",
      "             Researcher: Good morning.\n",
      "             Digital Athena: How can I help you today?\n",
      "             Researcher:' --interactive --color --batch_size 1024 --ctx_size 4096 \\\n",
      "             --keep -1 --temp 0 --mirostat 2 --in-prefix ' ' --interactive-first \\\n",
      "             --in-suffix 'Digital Athena:' --reverse-prompt 'Researcher:'\n",
      "\n",
      "       Here's an example of how you can use llamafile to summarize HTML URLs:\n",
      "\n",
      "             (\n",
      "               echo '[INST]Summarize the following text:'\n",
      "               links -codepage utf-8 \\\n",
      "                     -force-html \\\n",
      "                     -width 500 \\\n",
      "                     -dump https://www.poetryfoundation.org/poems/48860/the-raven |\n",
      "                 sed 's/   */ /g'\n",
      "               echo '[/INST]'\n",
      "             ) | llamafile \\\n",
      "                   -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \\\n",
      "                   -f /dev/stdin \\\n",
      "                   -c 0 \\\n",
      "                   --temp 0 \\\n",
      "                   -n 500 \\\n",
      "                   --no-display-prompt 2>/dev/null\n",
      "\n",
      "       Here's how you can use llamafile to describe a jpg/png/gif/bmp image:\n",
      "\n",
      "             llamafile --temp 0 \\\n",
      "               --image lemurs.jpg \\\n",
      "               -m llava-v1.5-7b-Q4_K.gguf \\\n",
      "               --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \\\n",
      "               -e -p '### User: What do you see?\\n### Assistant: ' \\\n",
      "               --no-display-prompt 2>/dev/null\n",
      "\n",
      "       If  you  wanted  to  write a script to rename all your image files, you\n",
      "       could use the following command to generate a safe filename:\n",
      "\n",
      "             llamafile --temp 0 \\\n",
      "                 --image ~/Pictures/lemurs.jpg \\\n",
      "                 -m llava-v1.5-7b-Q4_K.gguf \\\n",
      "                 --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \\\n",
      "                 --grammar 'root ::= [a-z]+ (\" \" [a-z]+)+' \\\n",
      "                 -e -p '### User: The image has...\\n### Assistant: ' \\\n",
      "                 --no-display-prompt 2>/dev/null |\n",
      "               sed -e's/ /_/g' -e's/$/.jpg/'\n",
      "             three_baby_lemurs_on_the_back_of_an_adult_lemur.jpg\n",
      "\n",
      "       Here's an example of how to make an API request to the OpenAI API  com‐\n",
      "       patible  completions  endpoint  when  your  \u001b[1mllamafile \u001b[22mis running in the\n",
      "       background in \u001b[1m--server \u001b[22mmode.\n",
      "\n",
      "             curl -s http://localhost:8080/v1/chat/completions \\\n",
      "                  -H \"Content-Type: application/json\" -d '{\n",
      "               \"model\": \"gpt-3.5-turbo\",\n",
      "               \"stream\": true,\n",
      "               \"messages\": [\n",
      "                 {\n",
      "                   \"role\": \"system\",\n",
      "                   \"content\": \"You are a poetic assistant.\"\n",
      "                 },\n",
      "                 {\n",
      "                   \"role\": \"user\",\n",
      "                   \"content\": \"Compose a poem that explains FORTRAN.\"\n",
      "                 }\n",
      "               ]\n",
      "             }' | python3 -c '\n",
      "             import json\n",
      "             import sys\n",
      "             json.dump(json.load(sys.stdin), sys.stdout, indent=2)\n",
      "             print()\n",
      "\n",
      "\u001b[1mPROTIP\u001b[0m\n",
      "       The \u001b[1m-ngl \u001b[4m\u001b[22m35\u001b[24m flag needs to be passed in order to use GPUs made by NVIDIA\n",
      "       and AMD.  It's not enabled by default since it sometimes  needs  to  be\n",
      "       tuned  based on the system hardware and model architecture, in order to\n",
      "       achieve optimal performance, and avoid compromising a shared display.\n",
      "\n",
      "\u001b[1mSEE ALSO\u001b[0m\n",
      "       \u001b[4mllamafile-quantize\u001b[24m(1),   \u001b[4mllamafile-perplexity\u001b[24m(1),    \u001b[4mllava-quantize\u001b[24m(1),\n",
      "       \u001b[4mzipalign\u001b[24m(1), \u001b[4munzip\u001b[24m(1)\n",
      "\n",
      "Mozilla Ocho                    January 1, 2024                   \u001b[4mLLAMAFILE\u001b[24m(1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "llamafile-0.8.13 --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280e890a-d7eb-4ef8-aecb-a87ef14dfd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1163  100  1163    0     0   2539      0 --:--:-- --:--:-- --:--:--  2544\n",
      "100 4166M  100 4166M    0     0  23.2M      0  0:02:59  0:02:59 --:--:-- 24.6M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "URL=https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "curl --location --output ../models/ggufs/mistral-7b-instruct-v0.1.Q4_K_M.gguf $URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392cf011-9ca4-4848-b8ff-2b1b63faa488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8553208\n",
      "-rw-r--r--  1 pughdr  KAUST\\Domain Users   4.1G Sep 19 10:41 mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "ls -lh ../models/ggufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a12614-cfa0-4a26-b117-ce9207a7d281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting /zip/llama.cpp/ggml.h to /Users/pughdr/.llamafile/v/0.8.13/ggml.h\n",
      "extracting /zip/llamafile/llamafile.h to /Users/pughdr/.llamafile/v/0.8.13/llamafile.h\n",
      "extracting /zip/llama.cpp/ggml-impl.h to /Users/pughdr/.llamafile/v/0.8.13/ggml-impl.h\n",
      "extracting /zip/llama.cpp/ggml-metal.h to /Users/pughdr/.llamafile/v/0.8.13/ggml-metal.h\n",
      "extracting /zip/llama.cpp/ggml-alloc.h to /Users/pughdr/.llamafile/v/0.8.13/ggml-alloc.h\n",
      "extracting /zip/llama.cpp/ggml-common.h to /Users/pughdr/.llamafile/v/0.8.13/ggml-common.h\n",
      "extracting /zip/llama.cpp/ggml-quants.h to /Users/pughdr/.llamafile/v/0.8.13/ggml-quants.h\n",
      "extracting /zip/llama.cpp/ggml-backend.h to /Users/pughdr/.llamafile/v/0.8.13/ggml-backend.h\n",
      "extracting /zip/llama.cpp/ggml-metal.metal to /Users/pughdr/.llamafile/v/0.8.13/ggml-metal.metal\n",
      "extracting /zip/llama.cpp/ggml-backend-impl.h to /Users/pughdr/.llamafile/v/0.8.13/ggml-backend-impl.h\n",
      "extracting /zip/llama.cpp/ggml-metal.m to /Users/pughdr/.llamafile/v/0.8.13/ggml-metal.m\n",
      "building ggml-metal.dylib with xcode...\n",
      "llamafile_log_command: cc -I. -O3 -fPIC -shared -pthread -DNDEBUG -ffixed-x28 -DTARGET_OS_OSX -DGGML_MULTIPLATFORM /Users/pughdr/.llamafile/v/0.8.13/ggml-metal.m -o /Users/pughdr/.llamafile/v/0.8.13/ggml-metal.dylib.bbd007 -framework Foundation -framework Metal -framework MetalKit\n",
      "Apple Metal GPU support successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"build\":1500,\"commit\":\"a30b324\",\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2841,\"msg\":\"build info\",\"tid\":\"34405054888\",\"timestamp\":1726731804}\n",
      "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2844,\"msg\":\"system info\",\"n_threads\":4,\"n_threads_batch\":-1,\"system_info\":\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \",\"tid\":\"34405054888\",\"timestamp\":1726731804,\"total_threads\":8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/ggufs/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.32 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  3992.52 MiB, ( 3992.58 / 16384.02)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  3992.51 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/pughdr/.llamafile/v/0.8.13/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =  2144.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":491,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"34405054888\",\"timestamp\":1726731813}\n",
      "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":500,\"msg\":\"new slot\",\"n_ctx_slot\":32768,\"slot_id\":0,\"tid\":\"34405054888\",\"timestamp\":1726731813}\n",
      "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":3062,\"msg\":\"model loaded\",\"tid\":\"34405054888\",\"timestamp\":1726731813}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama server listening at http://127.0.0.1:8080\n",
      "\n",
      "opening browser tab... (pass --nobrowser to disable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"function\":\"server_cli\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3185,\"msg\":\"HTTP server listening\",\"port\":\"8080\",\"tid\":\"34405054888\",\"timestamp\":1726731814}\n",
      "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1661,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"34405054888\",\"timestamp\":1726731814}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/manifest.json\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60695,\"status\":404,\"tid\":\"4577775312\",\"timestamp\":1726731814}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/_app/immutable/chunks/entry.l_Q2iNLf.js\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60695,\"status\":404,\"tid\":\"4577775312\",\"timestamp\":1726731815}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/opensearch.xml\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60696,\"status\":404,\"tid\":\"4577776496\",\"timestamp\":1726731815}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89F9Bx\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60697,\"status\":404,\"tid\":\"4577777728\",\"timestamp\":1726731817}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89F9C7\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60697,\"status\":404,\"tid\":\"4577777728\",\"timestamp\":1726731817}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89F9C9\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60697,\"status\":404,\"tid\":\"4577777728\",\"timestamp\":1726731817}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FAQK\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60700,\"status\":404,\"tid\":\"4577778864\",\"timestamp\":1726731822}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FBee\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60701,\"status\":404,\"tid\":\"4577780096\",\"timestamp\":1726731827}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FCsz\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60703,\"status\":404,\"tid\":\"4577781232\",\"timestamp\":1726731832}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FCt6\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60703,\"status\":404,\"tid\":\"4577781232\",\"timestamp\":1726731832}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/manifest.json\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60704,\"status\":404,\"tid\":\"4577782464\",\"timestamp\":1726731835}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/_app/immutable/chunks/entry.l_Q2iNLf.js\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60704,\"status\":404,\"tid\":\"4577782464\",\"timestamp\":1726731835}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/opensearch.xml\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60705,\"status\":404,\"tid\":\"4577783600\",\"timestamp\":1726731836}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FE5I\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60707,\"status\":404,\"tid\":\"4577776496\",\"timestamp\":1726731837}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FE5T\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60707,\"status\":404,\"tid\":\"4577776496\",\"timestamp\":1726731837}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FE5W\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60707,\"status\":404,\"tid\":\"4577776496\",\"timestamp\":1726731837}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FE5Z\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60707,\"status\":404,\"tid\":\"4577776496\",\"timestamp\":1726731837}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{\"EIO\":\"4\",\"t\":\"P89FFJl\",\"transport\":\"polling\"},\"path\":\"/ws/socket.io/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60708,\"status\":404,\"tid\":\"4577775312\",\"timestamp\":1726731842}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "llamafile-0.8.13 --model ../models/ggufs/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02854b51-a208-46dd-bab1-8cf62878837c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52722c-5943-4da4-b91e-8eb1b8161491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
