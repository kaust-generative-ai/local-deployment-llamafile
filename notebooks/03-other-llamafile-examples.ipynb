{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4f5461-7d28-40cc-9eac-34c62e7309d7",
   "metadata": {},
   "source": [
    "## Other example llamafiles\n",
    "\n",
    "Mozilla also provide example llamafiles for other models, so you can easily try out llamafile with different kinds of LLMs.\n",
    "\n",
    "| Model                  | Size     | License  | llamafile        | other quants                                      |\n",
    "| ---                    | ---      | ---      | ---              | ---                                               |\n",
    "| LLaVA 1.5              | 3.97 GB  | [LLaMA 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)  | [llava-v1.5-7b-q4.llamafile](https://huggingface.co/Mozilla/llava-v1.5-7b-llamafile/resolve/main/llava-v1.5-7b-q4.llamafile?download=true)                                                          | [See HF repo](https://huggingface.co/Mozilla/llava-v1.5-7b-llamafile)                    |\n",
    "| TinyLlama-1.1B         | 2.05 GB  | [Apache 2.0](https://choosealicense.com/licenses/apache-2.0/)                                                                      | [TinyLlama-1.1B-Chat-v1.0.F16.llamafile](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile?download=true)                       | [See HF repo](https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile)         |\n",
    "| Mistral-7B-Instruct    | 3.85 GB  | [Apache 2.0](https://choosealicense.com/licenses/apache-2.0/)                                                                      | [mistral-7b-instruct-v0.2.Q4\\_0.llamafile](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true)               | [See HF repo](https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile)    |\n",
    "| Phi-3-mini-4k-instruct | 7.67 GB  | [Apache 2.0](https://huggingface.co/Mozilla/Phi-3-mini-4k-instruct-llamafile/blob/main/LICENSE)                                    | [Phi-3-mini-4k-instruct.F16.llamafile](https://huggingface.co/Mozilla/Phi-3-mini-4k-instruct-llamafile/resolve/main/Phi-3-mini-4k-instruct.F16.llamafile?download=true)                        | [See HF repo](https://huggingface.co/Mozilla/Phi-3-mini-4k-instruct-llamafile)      |\n",
    "| Mixtral-8x7B-Instruct  | 30.03 GB | [Apache 2.0](https://choosealicense.com/licenses/apache-2.0/)                                                                      | [mixtral-8x7b-instruct-v0.1.Q5\\_K\\_M.llamafile](https://huggingface.co/Mozilla/Mixtral-8x7B-Instruct-v0.1-llamafile/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile?download=true)    | [See HF repo](https://huggingface.co/Mozilla/Mixtral-8x7B-Instruct-v0.1-llamafile)  |\n",
    "| WizardCoder-Python-34B | 22.23 GB | [LLaMA 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)                                                     | [wizardcoder-python-34b-v1.0.Q5\\_K\\_M.llamafile](https://huggingface.co/Mozilla/WizardCoder-Python-34B-V1.0-llamafile/resolve/main/wizardcoder-python-34b-v1.0.Q5_K_M.llamafile?download=true) | [See HF repo](https://huggingface.co/Mozilla/WizardCoder-Python-34B-V1.0-llamafile) |\n",
    "| WizardCoder-Python-13B | 7.33 GB  | [LLaMA 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)                                                     | [wizardcoder-python-13b.llamafile](https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b.llamafile?download=true)                                          | [See HF repo](https://huggingface.co/jartine/wizardcoder-13b-python)                |\n",
    "| LLaMA-3-Instruct-70B   | 37.25 GB | [llama3](https://huggingface.co/Mozilla/Meta-Llama-3-8B-Instruct-llamafile/blob/main/Meta-Llama-3-Community-License-Agreement.txt) | [Meta-Llama-3-70B-Instruct.Q4\\_0.llamafile](https://huggingface.co/Mozilla/Meta-Llama-3-70B-Instruct-llamafile/resolve/main/Meta-Llama-3-70B-Instruct.Q4_0.llamafile?download=true)            | [See HF repo](https://huggingface.co/Mozilla/Meta-Llama-3-70B-Instruct-llamafile)   |\n",
    "| LLaMA-3-Instruct-8B    | 5.37 GB  | [llama3](https://huggingface.co/Mozilla/Meta-Llama-3-8B-Instruct-llamafile/blob/main/Meta-Llama-3-Community-License-Agreement.txt) | [Meta-Llama-3-8B-Instruct.Q5\\_K\\_M.llamafile](https://huggingface.co/Mozilla/Meta-Llama-3-8B-Instruct-llamafile/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile?download=true)          | [See HF repo](https://huggingface.co/Mozilla/Meta-Llama-3-8B-Instruct-llamafile)    |\n",
    "| Rocket-3B              | 1.89 GB  | [cc-by-sa-4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.en)                                                             | [rocket-3b.Q5\\_K\\_M.llamafile](https://huggingface.co/Mozilla/rocket-3B-llamafile/resolve/main/rocket-3b.Q5_K_M.llamafile?download=true)                                                       | [See HF repo](https://huggingface.co/Mozilla/rocket-3B-llamafile)                   |\n",
    "| OLMo-7B              | 5.68 GB  | [Apache 2.0](https://huggingface.co/Mozilla/OLMo-7B-0424-llamafile/blob/main/LICENSE)                                                | [OLMo-7B-0424.Q6\\_K.llamafile](https://huggingface.co/Mozilla/OLMo-7B-0424-llamafile/resolve/main/OLMo-7B-0424.Q6_K.llamafile?download=true)                                                       | [See HF repo](https://huggingface.co/Mozilla/OLMo-7B-0424-llamafile)                   |\n",
    "| *Text Embedding Models* |          |                                                                                                                                    |                                                                                                                                                                                                |                                                                                     |\n",
    "| E5-Mistral-7B-Instruct  | 5.16 GB  | [MIT](https://choosealicense.com/licenses/mit/)                                                                                    | [e5-mistral-7b-instruct-Q5_K_M.llamafile](https://huggingface.co/Mozilla/e5-mistral-7b-instruct/resolve/main/e5-mistral-7b-instruct-Q5_K_M.llamafile?download=true)                            | [See HF repo](https://huggingface.co/Mozilla/e5-mistral-7b-instruct)                |\n",
    "| mxbai-embed-large-v1    | 0.7 GB   | [Apache 2.0](https://choosealicense.com/licenses/apache-2.0/)                                                                      | [mxbai-embed-large-v1-f16.llamafile](https://huggingface.co/Mozilla/mxbai-embed-large-v1-llamafile/resolve/main/mxbai-embed-large-v1-f16.llamafile?download=true)                              | [See HF Repo](https://huggingface.co/Mozilla/mxbai-embed-large-v1-llamafile)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b467277-7c6f-472b-a18a-0de6cc6adfb8",
   "metadata": {},
   "source": [
    "Here is an example for the Mistral command-line llamafile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfc375d-6303-4d79-bbf7-50ce05ca42fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1183  100  1183    0     0   2632      0 --:--:-- --:--:-- --:--:--  2634\n",
      "100 3952M  100 3952M    0     0  23.3M      0  0:02:49  0:02:49 --:--:-- 14.9M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "URL=https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true\n",
    "curl --output ../models/llamafiles/mistral-7b-instruct-v0.2.Q4_0.llamafile --location $URL \n",
    "\n",
    "chmod u+x ../models/llamafiles/mistral-7b-instruct-v0.2.Q4_0.llamafile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ee39de-7e7e-4edf-be41-cd88c99e6ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16483376\n",
      "-rwxr--r--  1 pughdr  KAUST\\Domain Users   4.0G Sep 19 08:15 \u001b[31mllava-v1.5-7b-q4.llamafile\u001b[m\u001b[m\n",
      "-rwxr--r--  1 pughdr  KAUST\\Domain Users   3.9G Sep 19 09:00 \u001b[31mmistral-7b-instruct-v0.2.Q4_0.llamafile\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lh ../models/llamafiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cee33a4-3630-435c-ac1c-df508b34e236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting /zip/llama.cpp/ggml.h to /Users/pughdr/.llamafile/v/0.8.5/ggml.h\n",
      "extracting /zip/llamafile/llamafile.h to /Users/pughdr/.llamafile/v/0.8.5/llamafile.h\n",
      "extracting /zip/llama.cpp/ggml-impl.h to /Users/pughdr/.llamafile/v/0.8.5/ggml-impl.h\n",
      "extracting /zip/llama.cpp/ggml-metal.h to /Users/pughdr/.llamafile/v/0.8.5/ggml-metal.h\n",
      "extracting /zip/llama.cpp/ggml-alloc.h to /Users/pughdr/.llamafile/v/0.8.5/ggml-alloc.h\n",
      "extracting /zip/llama.cpp/ggml-common.h to /Users/pughdr/.llamafile/v/0.8.5/ggml-common.h\n",
      "extracting /zip/llama.cpp/ggml-quants.h to /Users/pughdr/.llamafile/v/0.8.5/ggml-quants.h\n",
      "extracting /zip/llama.cpp/ggml-backend.h to /Users/pughdr/.llamafile/v/0.8.5/ggml-backend.h\n",
      "extracting /zip/llama.cpp/ggml-metal.metal to /Users/pughdr/.llamafile/v/0.8.5/ggml-metal.metal\n",
      "extracting /zip/llama.cpp/ggml-backend-impl.h to /Users/pughdr/.llamafile/v/0.8.5/ggml-backend-impl.h\n",
      "extracting /zip/llama.cpp/ggml-metal.m to /Users/pughdr/.llamafile/v/0.8.5/ggml-metal.m\n",
      "building ggml-metal.dylib with xcode...\n",
      "llamafile_log_command: cc -I. -O3 -fPIC -shared -pthread -DNDEBUG -ffixed-x28 -DTARGET_OS_OSX -DGGML_MULTIPLATFORM /Users/pughdr/.llamafile/v/0.8.5/ggml-metal.m -o /Users/pughdr/.llamafile/v/0.8.5/ggml-metal.dylib.6sr16x -framework Foundation -framework Metal -framework MetalKit\n",
      "Apple Metal GPU support successfully loaded\n",
      "main: llamafile version 0.8.5\n",
      "main: seed  = 1726728801\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from mistral-7b-instruct-v0.2.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   5:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   6:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   7:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = n/a\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  3847.58 MiB, ( 3847.64 / 16384.02)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  3847.57 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/pughdr/.llamafile/v/0.8.5/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.700\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 2048, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]Write a story about llamas[/INST] Once upon a time, in the high Andes Mountains of South America, there was a quaint little village nestled among the snow-capped peaks. This village was known far and wide for its unique and gentle inhabitants - the llamas.\n",
      "\n",
      "The villagers, who were mainly shepherds, had a deep connection with these majestic creatures. They had lived alongside them for generations, relying on their wool for warmth and their strength for transportation.\n",
      "\n",
      "One sunny afternoon, as the villagers went about their daily chores, a young shepherd boy named Mateo was playing with his favorite llama, Max. Max was a large and strong llama, with a thick coat of golden-brown wool and a gentle expression in his deep-set, dark eyes. Mateo and Max had an unbreakable bond. They spent their days together, with Mateo leading Max on long walks through the verdant pastures, and Max protecting Mateo from any harm that might come his way.\n",
      "\n",
      "As they played, Mateo noticed that Max seemed restless. Max kept looking towards the horizon, and his usual gentle demeanor had given way to a sense of urgency. Mateo, sensing something was amiss, followed Max's gaze and saw a group of dark clouds gathering in the distance.\n",
      "\n",
      "The villagers were taken by surprise when the storm struck. The winds howled, and the rains came down in torrents, flooding the pastures and destroying the crops. The villagers were frantic as they tried to save their homes and their animals from the raging elements.\n",
      "\n",
      "Max, sensing the danger, took it upon himself to protect his herd. He led them to higher ground, where they were safe from the floodwaters. The villagers, grateful for Max's bravery, praised him as a hero.\n",
      "\n",
      "Days passed, and the storm subsided. The villagers began to rebuild their homes and their lives. They were grateful for the llamas, who had provided them with warmth, food, and protection during the storm.\n",
      "\n",
      "Mateo and Max continued their bond, growing stronger with each passing day. And so, the story of the loyal and brave llamas of the Andes Mountains became a legend, passed down from generation to generation. The villagers knew that they could always rely on their beloved llamas to keep them safe and to help them through even the toughest of times.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [end of text]\n",
      "\n",
      "llama_print_timings:        load time =    8441.24 ms\n",
      "llama_print_timings:      sample time =      15.98 ms /   523 runs   (    0.03 ms per token, 32722.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     205.98 ms /    14 tokens (   14.71 ms per token,    67.97 tokens per second)\n",
      "llama_print_timings:        eval time =   26487.13 ms /   522 runs   (   50.74 ms per token,    19.71 tokens per second)\n",
      "llama_print_timings:       total time =   26765.54 ms /   536 tokens\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "../models/llamafiles/mistral-7b-instruct-v0.2.Q4_0.llamafile --temp 0.7 -p '[INST]Write a story about llamas[/INST]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dde967-b30b-4f2b-a4ea-5fd71e2a573f",
   "metadata": {},
   "source": [
    "And here is an example for WizardCoder-Python command-line llamafile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b572836-5a49-44ce-8950-6d7a391181cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1171  100  1171    0     0   3205      0 --:--:-- --:--:-- --:--:--  3208\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "URL=https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b.llamafile?download=true\n",
    "curl --output ../models/llamafiles/wizardcoder-python-13b.llamafile --location $URL \n",
    "\n",
    "chmod u+x ../models/llamafiles/wizardcoder-python-13b.llamafile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df730ab-0ce4-48da-a654-a9bd89487bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 34573176\n",
      "-rwxr--r--  1 pughdr  KAUST\\Domain Users   4.0G Sep 19 08:15 \u001b[31mllava-v1.5-7b-q4.llamafile\u001b[m\u001b[m\n",
      "-rwxr--r--  1 pughdr  KAUST\\Domain Users   3.9G Sep 19 09:00 \u001b[31mmistral-7b-instruct-v0.2.Q4_0.llamafile\u001b[m\u001b[m\n",
      "-rwxr--r--  1 pughdr  KAUST\\Domain Users   8.6G Sep 19 10:09 \u001b[31mwizardcoder-python-13b.llamafile\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lh ../models/llamafiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b10b53-6681-4ac6-9644-df846f91c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting /zip/llama.cpp/ggml.h to /Users/pughdr/.llamafile/v/0.8.7/ggml.h\n",
      "extracting /zip/llamafile/llamafile.h to /Users/pughdr/.llamafile/v/0.8.7/llamafile.h\n",
      "extracting /zip/llama.cpp/ggml-impl.h to /Users/pughdr/.llamafile/v/0.8.7/ggml-impl.h\n",
      "extracting /zip/llama.cpp/ggml-metal.h to /Users/pughdr/.llamafile/v/0.8.7/ggml-metal.h\n",
      "extracting /zip/llama.cpp/ggml-alloc.h to /Users/pughdr/.llamafile/v/0.8.7/ggml-alloc.h\n",
      "extracting /zip/llama.cpp/ggml-common.h to /Users/pughdr/.llamafile/v/0.8.7/ggml-common.h\n",
      "extracting /zip/llama.cpp/ggml-quants.h to /Users/pughdr/.llamafile/v/0.8.7/ggml-quants.h\n",
      "extracting /zip/llama.cpp/ggml-backend.h to /Users/pughdr/.llamafile/v/0.8.7/ggml-backend.h\n",
      "extracting /zip/llama.cpp/ggml-metal.metal to /Users/pughdr/.llamafile/v/0.8.7/ggml-metal.metal\n",
      "extracting /zip/llama.cpp/ggml-backend-impl.h to /Users/pughdr/.llamafile/v/0.8.7/ggml-backend-impl.h\n",
      "extracting /zip/llama.cpp/ggml-metal.m to /Users/pughdr/.llamafile/v/0.8.7/ggml-metal.m\n",
      "building ggml-metal.dylib with xcode...\n",
      "llamafile_log_command: cc -I. -O3 -fPIC -shared -pthread -DNDEBUG -ffixed-x28 -DTARGET_OS_OSX -DGGML_MULTIPLATFORM /Users/pughdr/.llamafile/v/0.8.7/ggml-metal.m -o /Users/pughdr/.llamafile/v/0.8.7/ggml-metal.dylib.h06z4t -framework Foundation -framework Metal -framework MetalKit\n",
      "Apple Metal GPU support successfully loaded\n",
      "main: llamafile version 0.8.7\n",
      "main: seed  = 1726729982\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from wizardcoder-python-13b-v1.0.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = wizardlm_wizardcoder-python-13b-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32001]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32001]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32001]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 260/32001 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32001\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = wizardlm_wizardcoder-python-13b-v1.0\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: PRE token        = 32007 ''\n",
      "llm_load_print_meta: SUF token        = 32008 ''\n",
      "llm_load_print_meta: MID token        = 32009 ''\n",
      "llm_load_print_meta: EOT token        = 32010 ''\n",
      "llm_load_tensors: ggml ctx size =    0.43 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  8694.23 MiB, ( 8694.30 / 16384.02)\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  8694.22 MiB\n",
      "llm_load_tensors:        CPU buffer size =   107.43 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/pughdr/.llamafile/v/0.8.7/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   400.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =    85.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    11.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```c\n",
      "void *memcpy_sse2(char *dst, const char *src, size_t size) {\n",
      "    size_t i;\n",
      "    __m128i *dst128 = (__m128i *)dst;\n",
      "    __m128i *src128 = (__m128i *)src;\n",
      "    size_t count = size / 16;\n",
      "    for (i = 0; i < count; i++) {\n",
      "        _mm_storeu_si128(dst128++, _mm_loadu_si128(src128++));\n",
      "    }\n",
      "    size %= 16;\n",
      "    if (size) {\n",
      "        memcpy(dst + i * 16, src + i * 16, size);\n",
      "    }\n",
      "    return dst;\n",
      "}\n",
      "```\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13666.24 ms\n",
      "llama_print_timings:      sample time =       1.72 ms /   173 runs   (    0.01 ms per token, 100581.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     460.19 ms /    29 tokens (   15.87 ms per token,    63.02 tokens per second)\n",
      "llama_print_timings:        eval time =   19419.14 ms /   172 runs   (  112.90 ms per token,     8.86 tokens per second)\n",
      "llama_print_timings:       total time =   19901.16 ms /   201 tokens\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "../models/llamafiles/wizardcoder-python-13b.llamafile --temp 0 -e -r '```\\n' -p '```c\\nvoid *memcpy_sse2(char *dst, const char *src, size_t size) {\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab72b02-060b-4f5e-a2c3-affd645c0a3d",
   "metadata": {},
   "source": [
    "And here's an example for the LLaVA command-line llamafile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b89296-9e9f-444b-bf49-32f24bc96a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "..models/llamafiles/llava-v1.5-7b-q4.llamafile --temp 0.2 --image lemurs.jpg -e -p '### User: What do you see?\\n### Assistant:'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2dda36-1261-43ec-bf8d-7bd2f88ccb89",
   "metadata": {},
   "source": [
    "Unfortunately, Windows users cannot make use of many of these example llamafiles because Windows has a maximum executable file size of 4GB, and all of these examples exceed that size. (The LLaVA llamafile works on Windows because it is 30MB shy of the size limit.) But don't lose heart: llamafile allows you to use external weights; this is described later in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52722c-5943-4da4-b91e-8eb1b8161491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
