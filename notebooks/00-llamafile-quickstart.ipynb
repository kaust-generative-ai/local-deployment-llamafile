{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54243d18-5a6a-4366-b22d-f346578a7a2b",
   "metadata": {},
   "source": [
    "# Llamafile Quickstart\n",
    "\n",
    "<img src=\"https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/llamafile-640x640.png?raw=true\" width=\"320\" height=\"320\" style=\"margin:auto\" alt=\"[line drawing of llama animal head in front of slightly open manilla folder filled with files]\">\n",
    "\n",
    "\n",
    "**llamafile lets you distribute and run LLMs with a single file. ([announcement blog post](https://hacks.mozilla.org/2023/11/introducing-llamafile/))**\n",
    "\n",
    "Our goal is to make open LLMs much more accessible to both developers and end users. We're doing that by\n",
    "combining [llama.cpp](https://github.com/ggerganov/llama.cpp) with [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into one framework that collapses all the complexity of LLMs down to a single-file executable (called a \"llamafile\") that runs locally on most computers, with no installation.<br/><br/>\n",
    "\n",
    "<a href=\"https://future.mozilla.org\"><img src=\"https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/mozilla-logo-bw-rgb.png?raw=true\" width=\"150\"></a><br/> llamafile is a Mozilla Builders project.<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7965b0-bdf0-4463-bc2a-48f9fc8476d6",
   "metadata": {},
   "source": [
    "The easiest way to try it for yourself is to download our example llamafile for the [LLaVA](https://llava-vl.github.io/) model. LLaVA is a new LLM that can do more than just chat; you can also upload images and ask it questions\n",
    "about them. With llamafile, this all happens locally; no data ever leaves your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67782052-d2ab-4e41-9ef0-a8b0d9050071",
   "metadata": {},
   "source": [
    "## Download a Llamafile..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7dfd5ed-1254-4fb5-a307-d6775f92b9bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: curl [options...] <url>\n",
      "     --abstract-unix-socket <path>   Connect via abstract Unix domain socket\n",
      "     --alt-svc <filename>            Enable alt-svc with this cache file\n",
      "     --anyauth                       Pick any authentication method\n",
      " -a, --append                        Append to target file when uploading\n",
      "     --aws-sigv4 <provider1[:prvdr2[:reg[:srv]]]>  AWS V4 signature auth\n",
      "     --basic                         HTTP Basic Authentication\n",
      "     --ca-native                     Load CA certs from the OS\n",
      "     --cacert <file>                 CA certificate to verify peer against\n",
      "     --capath <dir>                  CA directory to verify peer against\n",
      " -E, --cert <certificate[:password]>  Client certificate file and password\n",
      "     --cert-status                   Verify server cert status OCSP-staple\n",
      "     --cert-type <type>              Certificate type (DER/PEM/ENG/P12)\n",
      "     --ciphers <list of ciphers>     SSL ciphers to use\n",
      "     --compressed                    Request compressed response\n",
      "     --compressed-ssh                Enable SSH compression\n",
      " -K, --config <file>                 Read config from a file\n",
      "     --connect-timeout <seconds>     Maximum time allowed to connect\n",
      "     --connect-to <HOST1:PORT1:HOST2:PORT2>  Connect to host\n",
      " -C, --continue-at <offset>          Resumed transfer offset\n",
      " -b, --cookie <data|filename>        Send cookies from string/load from file\n",
      " -c, --cookie-jar <filename>         Save cookies to <filename> after operation\n",
      "     --create-dirs                   Create necessary local directory hierarchy\n",
      "     --create-file-mode <mode>       File mode for created files\n",
      "     --crlf                          Convert LF to CRLF in upload\n",
      "     --crlfile <file>                Certificate Revocation list\n",
      "     --curves <list>                 (EC) TLS key exchange algorithms to request\n",
      " -d, --data <data>                   HTTP POST data\n",
      "     --data-ascii <data>             HTTP POST ASCII data\n",
      "     --data-binary <data>            HTTP POST binary data\n",
      "     --data-raw <data>               HTTP POST data, '@' allowed\n",
      "     --data-urlencode <data>         HTTP POST data URL encoded\n",
      "     --delegation <LEVEL>            GSS-API delegation permission\n",
      "     --digest                        HTTP Digest Authentication\n",
      " -q, --disable                       Disable .curlrc\n",
      "     --disable-eprt                  Inhibit using EPRT or LPRT\n",
      "     --disable-epsv                  Inhibit using EPSV\n",
      "     --disallow-username-in-url      Disallow username in URL\n",
      "     --dns-interface <interface>     Interface to use for DNS requests\n",
      "     --dns-ipv4-addr <address>       IPv4 address to use for DNS requests\n",
      "     --dns-ipv6-addr <address>       IPv6 address to use for DNS requests\n",
      "     --dns-servers <addresses>       DNS server addrs to use\n",
      "     --doh-cert-status               Verify DoH server cert status OCSP-staple\n",
      "     --doh-insecure                  Allow insecure DoH server connections\n",
      "     --doh-url <URL>                 Resolve hostnames over DoH\n",
      " -D, --dump-header <filename>        Write the received headers to <filename>\n",
      "     --egd-file <file>               EGD socket path for random data\n",
      "     --engine <name>                 Crypto engine to use\n",
      "     --etag-compare <file>           Load ETag from file\n",
      "     --etag-save <file>              Parse incoming ETag and save to a file\n",
      "     --expect100-timeout <seconds>   How long to wait for 100-continue\n",
      " -f, --fail                          Fail fast with no output on HTTP errors\n",
      "     --fail-early                    Fail on first transfer error\n",
      "     --fail-with-body                Fail on HTTP errors but save the body\n",
      "     --false-start                   Enable TLS False Start\n",
      " -F, --form <name=content>           Specify multipart MIME data\n",
      "     --form-escape                   Escape form fields using backslash\n",
      "     --form-string <name=string>     Specify multipart MIME data\n",
      "     --ftp-account <data>            Account data string\n",
      "     --ftp-alternative-to-user <command>  String to replace USER [name]\n",
      "     --ftp-create-dirs               Create the remote dirs if not present\n",
      "     --ftp-method <method>           Control CWD usage\n",
      "     --ftp-pasv                      Send PASV/EPSV instead of PORT\n",
      " -P, --ftp-port <address>            Send PORT instead of PASV\n",
      "     --ftp-pret                      Send PRET before PASV\n",
      "     --ftp-skip-pasv-ip              Skip the IP address for PASV\n",
      "     --ftp-ssl-ccc                   Send CCC after authenticating\n",
      "     --ftp-ssl-ccc-mode <active/passive>  Set CCC mode\n",
      "     --ftp-ssl-control               Require TLS for login, clear for transfer\n",
      " -G, --get                           Put the post data in the URL and use GET\n",
      " -g, --globoff                       Disable URL globbing with {} and []\n",
      "     --happy-eyeballs-timeout-ms <ms>  Time for IPv6 before IPv4\n",
      "     --haproxy-clientip <ip>         Set address in HAProxy PROXY\n",
      "     --haproxy-protocol              Send HAProxy PROXY protocol v1 header\n",
      " -I, --head                          Show document info only\n",
      " -H, --header <header/@file>         Pass custom header(s) to server\n",
      " -h, --help <category>               Get help for commands\n",
      "     --hostpubmd5 <md5>              Acceptable MD5 hash of host public key\n",
      "     --hostpubsha256 <sha256>        Acceptable SHA256 hash of host public key\n",
      "     --hsts <filename>               Enable HSTS with this cache file\n",
      "     --http0.9                       Allow HTTP 0.9 responses\n",
      " -0, --http1.0                       Use HTTP 1.0\n",
      "     --http1.1                       Use HTTP 1.1\n",
      "     --http2                         Use HTTP/2\n",
      "     --http2-prior-knowledge         Use HTTP 2 without HTTP/1.1 Upgrade\n",
      "     --http3                         Use HTTP v3\n",
      "     --http3-only                    Use HTTP v3 only\n",
      "     --ignore-content-length         Ignore the size of the remote resource\n",
      " -i, --include                       Include response headers in output\n",
      " -k, --insecure                      Allow insecure server connections\n",
      "     --interface <name>              Use network INTERFACE (or address)\n",
      "     --ipfs-gateway <URL>            Gateway for IPFS\n",
      " -4, --ipv4                          Resolve names to IPv4 addresses\n",
      " -6, --ipv6                          Resolve names to IPv6 addresses\n",
      "     --json <data>                   HTTP POST JSON\n",
      " -j, --junk-session-cookies          Ignore session cookies read from file\n",
      "     --keepalive-time <seconds>      Interval time for keepalive probes\n",
      "     --key <key>                     Private key filename\n",
      "     --key-type <type>               Private key file type (DER/PEM/ENG)\n",
      "     --krb <level>                   Enable Kerberos with security <level>\n",
      "     --libcurl <file>                Generate libcurl code for this command line\n",
      "     --limit-rate <speed>            Limit transfer speed to RATE\n",
      " -l, --list-only                     List only mode\n",
      "     --local-port <range>            Use a local port number within RANGE\n",
      " -L, --location                      Follow redirects\n",
      "     --location-trusted            Like --location, but send auth to other hosts\n",
      "     --login-options <options>       Server login options\n",
      "     --mail-auth <address>           Originator address of the original email\n",
      "     --mail-from <address>           Mail from this address\n",
      "     --mail-rcpt <address>           Mail to this address\n",
      "     --mail-rcpt-allowfails          Allow RCPT TO command to fail\n",
      " -M, --manual                        Display the full manual\n",
      "     --max-filesize <bytes>          Maximum file size to download\n",
      "     --max-redirs <num>              Maximum number of redirects allowed\n",
      " -m, --max-time <seconds>            Maximum time allowed for transfer\n",
      "     --metalink                      Process given URLs as metalink XML file\n",
      "     --negotiate                     Use HTTP Negotiate (SPNEGO) authentication\n",
      " -n, --netrc                         Must read .netrc for username and password\n",
      "     --netrc-file <filename>         Specify FILE for netrc\n",
      "     --netrc-optional                Use either .netrc or URL\n",
      " -:, --next                        Make next URL use its separate set of options\n",
      "     --no-alpn                       Disable the ALPN TLS extension\n",
      " -N, --no-buffer                     Disable buffering of the output stream\n",
      "     --no-clobber                    Do not overwrite files that already exist\n",
      "     --no-keepalive                  Disable TCP keepalive on the connection\n",
      "     --no-npn                        Disable the NPN TLS extension\n",
      "     --no-progress-meter             Do not show the progress meter\n",
      "     --no-sessionid                  Disable SSL session-ID reusing\n",
      "     --noproxy <no-proxy-list>       List of hosts which do not use proxy\n",
      "     --ntlm                          HTTP NTLM authentication\n",
      "     --ntlm-wb                       HTTP NTLM authentication with winbind\n",
      "     --oauth2-bearer <token>         OAuth 2 Bearer Token\n",
      " -o, --output <file>                 Write to file instead of stdout\n",
      "     --output-dir <dir>              Directory to save files in\n",
      " -Z, --parallel                      Perform transfers in parallel\n",
      "     --parallel-immediate         Do not wait for multiplexing (with --parallel)\n",
      "     --parallel-max <num>            Maximum concurrency for parallel transfers\n",
      "     --pass <phrase>                 Pass phrase for the private key\n",
      "     --path-as-is                    Do not squash .. sequences in URL path\n",
      "     --pinnedpubkey <hashes>       FILE/HASHES Public key to verify peer against\n",
      "     --post301                       Do not switch to GET after a 301 redirect\n",
      "     --post302                       Do not switch to GET after a 302 redirect\n",
      "     --post303                       Do not switch to GET after a 303 redirect\n",
      "     --preproxy [protocol://]host[:port]  Use this proxy first\n",
      " -#, --progress-bar                  Display transfer progress as a bar\n",
      "     --proto <protocols>             Enable/disable PROTOCOLS\n",
      "     --proto-default <protocol>      Use PROTOCOL for any URL missing a scheme\n",
      "     --proto-redir <protocols>       Enable/disable PROTOCOLS on redirect\n",
      " -x, --proxy [protocol://]host[:port]  Use this proxy\n",
      "     --proxy-anyauth                 Pick any proxy authentication method\n",
      "     --proxy-basic                   Use Basic authentication on the proxy\n",
      "     --proxy-ca-native               Load CA certs from the OS to verify proxy\n",
      "     --proxy-cacert <file>           CA certificates to verify proxy against\n",
      "     --proxy-capath <dir>            CA directory to verify proxy against\n",
      "     --proxy-cert <cert[:passwd]>    Set client certificate for proxy\n",
      "     --proxy-cert-type <type>        Client certificate type for HTTPS proxy\n",
      "     --proxy-ciphers <list>          SSL ciphers to use for proxy\n",
      "     --proxy-crlfile <file>          Set a CRL list for proxy\n",
      "     --proxy-digest                  Digest auth with the proxy\n",
      "     --proxy-header <header/@file>   Pass custom header(s) to proxy\n",
      "     --proxy-http2                   Use HTTP/2 with HTTPS proxy\n",
      "     --proxy-insecure                Skip HTTPS proxy cert verification\n",
      "     --proxy-key <key>               Private key for HTTPS proxy\n",
      "     --proxy-key-type <type>         Private key file type for proxy\n",
      "     --proxy-negotiate               HTTP Negotiate (SPNEGO) auth with the proxy\n",
      "     --proxy-ntlm                    NTLM authentication with the proxy\n",
      "     --proxy-pass <phrase>       Pass phrase for the private key for HTTPS proxy\n",
      "     --proxy-pinnedpubkey <hashes>   FILE/HASHES public key to verify proxy with\n",
      "     --proxy-service-name <name>     SPNEGO proxy service name\n",
      "     --proxy-ssl-allow-beast     Allow security flaw for interop for HTTPS proxy\n",
      "     --proxy-ssl-auto-client-cert    Auto client certificate for proxy\n",
      "     --proxy-tls13-ciphers <ciphersuite list>  TLS 1.3 proxy cipher suites\n",
      "     --proxy-tlsauthtype <type>      TLS authentication type for HTTPS proxy\n",
      "     --proxy-tlspassword <string>    TLS password for HTTPS proxy\n",
      "     --proxy-tlsuser <name>          TLS username for HTTPS proxy\n",
      "     --proxy-tlsv1                   TLSv1 for HTTPS proxy\n",
      " -U, --proxy-user <user:password>    Proxy user and password\n",
      "     --proxy1.0 <host[:port]>        Use HTTP/1.0 proxy on given port\n",
      " -p, --proxytunnel                   HTTP proxy tunnel (using CONNECT)\n",
      "     --pubkey <key>                  SSH Public key filename\n",
      " -Q, --quote <command>               Send command(s) to server before transfer\n",
      "     --random-file <file>            File for reading random data from\n",
      " -r, --range <range>                 Retrieve only the bytes within RANGE\n",
      "     --rate <max request rate>       Request rate for serial transfers\n",
      "     --raw                           Do HTTP raw; no transfer decoding\n",
      " -e, --referer <URL>                 Referrer URL\n",
      " -J, --remote-header-name            Use the header-provided filename\n",
      " -O, --remote-name                   Write output to file named as remote file\n",
      "     --remote-name-all               Use the remote filename for all URLs\n",
      " -R, --remote-time                   Set remote file's time on local output\n",
      "     --remove-on-error               Remove output file on errors\n",
      " -X, --request <method>              Specify request method to use\n",
      "     --request-target <path>         Specify the target for this request\n",
      "     --resolve <[+]host:port:addr[,addr]...>  Resolve host+port to address\n",
      "     --retry <num>                   Retry request if transient problems occur\n",
      "     --retry-all-errors              Retry all errors (with --retry)\n",
      "     --retry-connrefused             Retry on connection refused (with --retry)\n",
      "     --retry-delay <seconds>         Wait time between retries\n",
      "     --retry-max-time <seconds>      Retry only within this period\n",
      "     --sasl-authzid <identity>       Identity for SASL PLAIN authentication\n",
      "     --sasl-ir                       Initial response in SASL authentication\n",
      "     --service-name <name>           SPNEGO service name\n",
      " -S, --show-error                    Show error even when -s is used\n",
      " -s, --silent                        Silent mode\n",
      "     --socks4 <host[:port]>          SOCKS4 proxy on given host + port\n",
      "     --socks4a <host[:port]>         SOCKS4a proxy on given host + port\n",
      "     --socks5 <host[:port]>          SOCKS5 proxy on given host + port\n",
      "     --socks5-basic                  Username/password auth for SOCKS5 proxies\n",
      "     --socks5-gssapi                 Enable GSS-API auth for SOCKS5 proxies\n",
      "     --socks5-gssapi-nec             Compatibility with NEC SOCKS5 server\n",
      "     --socks5-gssapi-service <name>  SOCKS5 proxy service name for GSS-API\n",
      "     --socks5-hostname <host[:port]>  SOCKS5 proxy, pass hostname to proxy\n",
      " -Y, --speed-limit <speed>           Stop transfers slower than this\n",
      " -y, --speed-time <seconds>          Trigger 'speed-limit' abort after this time\n",
      "     --ssl                           Try enabling TLS\n",
      "     --ssl-allow-beast               Allow security flaw to improve interop\n",
      "     --ssl-auto-client-cert          Use auto client certificate (Schannel)\n",
      "     --ssl-no-revoke                 Disable cert revocation checks (Schannel)\n",
      "     --ssl-reqd                      Require SSL/TLS\n",
      "     --ssl-revoke-best-effort        Ignore missing cert CRL dist points\n",
      " -2, --sslv2                         SSLv2\n",
      " -3, --sslv3                         SSLv3\n",
      "     --stderr <file>                 Where to redirect stderr\n",
      "     --styled-output                 Enable styled output for HTTP headers\n",
      "     --suppress-connect-headers      Suppress proxy CONNECT response headers\n",
      "     --tcp-fastopen                  Use TCP Fast Open\n",
      "     --tcp-nodelay                   Set TCP_NODELAY\n",
      " -t, --telnet-option <opt=val>       Set telnet option\n",
      "     --tftp-blksize <value>          Set TFTP BLKSIZE option\n",
      "     --tftp-no-options               Do not send any TFTP options\n",
      " -z, --time-cond <time>              Transfer based on a time condition\n",
      "     --tls-max <VERSION>             Maximum allowed TLS version\n",
      "     --tls13-ciphers <list>          TLS 1.3 cipher suites to use\n",
      "     --tlsauthtype <type>            TLS authentication type\n",
      "     --tlspassword <string>          TLS password\n",
      "     --tlsuser <name>                TLS username\n",
      " -1, --tlsv1                         TLSv1.0 or greater\n",
      "     --tlsv1.0                       TLSv1.0 or greater\n",
      "     --tlsv1.1                       TLSv1.1 or greater\n",
      "     --tlsv1.2                       TLSv1.2 or greater\n",
      "     --tlsv1.3                       TLSv1.3 or greater\n",
      "     --tr-encoding                   Request compressed transfer encoding\n",
      "     --trace <file>                  Write a debug trace to FILE\n",
      "     --trace-ascii <file>            Like --trace, but without hex output\n",
      "     --trace-config <string>         Details to log in trace/verbose output\n",
      "     --trace-ids                     Transfer + connection ids in verbose output\n",
      "     --trace-time                    Add time stamps to trace/verbose output\n",
      "     --unix-socket <path>            Connect through this Unix domain socket\n",
      " -T, --upload-file <file>            Transfer local FILE to destination\n",
      "     --url <url>                     URL to work with\n",
      "     --url-query <data>              Add a URL query part\n",
      " -B, --use-ascii                     Use ASCII/text transfer\n",
      " -u, --user <user:password>          Server user and password\n",
      " -A, --user-agent <name>             Send User-Agent <name> to server\n",
      "     --variable <[%]name=text/@file>  Set variable\n",
      " -v, --verbose                       Make the operation more talkative\n",
      " -V, --version                       Show version number and quit\n",
      " -w, --write-out <format>            Output FORMAT after completion\n",
      "     --xattr                         Store metadata in extended file attributes\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl --help all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be537377-8ac5-42bd-8906-37dd34badc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1164  100  1164    0     0   2477      0 --:--:-- --:--:-- --:--:--  2481\n",
      "100 6520M  100 6520M    0     0  13.3M      0  0:08:09  0:08:09 --:--:-- 16.0M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "URL=https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/resolve/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile\n",
    "curl --output ../models/llamafiles/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile --location $URL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77143039-2451-44a6-8a85-2783375ee071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13371320\n",
      "-rw-r--r--  1 pughdr  KAUST\\Domain Users   6.4G Sep 26 11:36 Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lh ../models/llamafiles/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e645a-8530-41d5-86a8-4fe89fb337a5",
   "metadata": {},
   "source": [
    "If you're using macOS, Linux, or BSD, you'll need to grant permission for your computer to execute this new file. (You only need to do this once.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639b0476-0c41-44eb-801a-7f2eaf105ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "chmod u+x ../models/llamafiles/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5b3909-f2b6-4b23-947f-f5bae7847de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13371320\n",
      "-rwxr--r--  1 pughdr  KAUST\\Domain Users   6.4G Sep 26 11:36 \u001b[31mMeta-Llama-3.1-8B-Instruct.Q6_K.llamafile\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lh ../models/llamafiles/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c7b32-21f5-4dba-a448-0f77e101ef43",
   "metadata": {},
   "source": [
    "## Run the Llamafile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d0a840-5c49-4970-84c2-1b9b32cc86e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apple Metal GPU support successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"build\":1500,\"commit\":\"a30b324\",\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2841,\"msg\":\"build info\",\"tid\":\"34405054888\",\"timestamp\":1727339975}\n",
      "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2844,\"msg\":\"system info\",\"n_threads\":4,\"n_threads_batch\":-1,\"system_info\":\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \",\"tid\":\"34405054888\",\"timestamp\":1727339975,\"total_threads\":8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 292 tensors from Meta-Llama-3.1-8B-Instruct.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   3:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   4:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   5:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 6.14 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = n/a\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.32 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  6282.97 MiB, ( 6283.03 / 16384.02)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  6282.96 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6282.97 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/pughdr/.llamafile/v/0.8.13/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   250.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":491,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"34405054888\",\"timestamp\":1727339976}\n",
      "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":500,\"msg\":\"new slot\",\"n_ctx_slot\":8192,\"slot_id\":0,\"tid\":\"34405054888\",\"timestamp\":1727339976}\n",
      "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":3062,\"msg\":\"model loaded\",\"tid\":\"34405054888\",\"timestamp\":1727339976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama server listening at http://127.0.0.1:8080\n",
      "\n",
      "opening browser tab... (pass --nobrowser to disable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"function\":\"server_cli\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3185,\"msg\":\"HTTP server listening\",\"port\":\"8080\",\"tid\":\"34405054888\",\"timestamp\":1727339976}\n",
      "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1661,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"34405054888\",\"timestamp\":1727339976}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49598,\"status\":200,\"tid\":\"4295967872\",\"timestamp\":1727339976}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/index.js\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49598,\"status\":200,\"tid\":\"4295967872\",\"timestamp\":1727339976}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/completion.js\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49599,\"status\":200,\"tid\":\"4295969072\",\"timestamp\":1727339976}\n",
      "{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/json-schema-to-grammar.mjs\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49600,\"status\":200,\"tid\":\"4295970368\",\"timestamp\":1727339976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "../models/llamafiles/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f7b92-6cf1-4155-962d-b119691228df",
   "metadata": {},
   "source": [
    "Your browser should open automatically and display a chat interface. If it doesn't, just open your browser and point it at http://localhost:8080.\n",
    "\n",
    "When you're done chatting, return to your terminal and hit `Control-C` to shut down llamafile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c37be-9360-4913-a7d1-51c08b6109f2",
   "metadata": {},
   "source": [
    "## How Llamafile works\n",
    "\n",
    "A Llamafile is an *executable* LLM that you can run on your own computer. It contains the weights for a given open LLM, as well as everything needed to actually run that model on your computer. There's nothing to install or configure (with a few caveats, discussed in subsequent sections of this tutorial).\n",
    "\n",
    "This is all accomplished by combining [LLaMA C++](https://github.com/ggerganov/llama.cpp) with [Cosmopolitan Libc](https://github.com/jart/cosmopolitan), which provides some useful capabilities:\n",
    "\n",
    "1. Llamafiles can run on multiple CPU *microarchitectures*. Mozilla devs added runtime dispatching to LLaMA C++ that lets new Intel systems use modern CPU features without trading away support for older computers.\n",
    "\n",
    "2. Llamafiles can run on multiple CPU *architectures*. Mozilla devs concatenated AMD64 and ARM64 builds with a shell script that launches the appropriate one. The file format is compatible with WIN32 and most UNIX shells. It's also able to be easily converted (by either you or your users) to the platform-native format, whenever required.\n",
    "\n",
    "3. Llamafiles can run on six OSes (macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD). If you make your own llama files, you'll only need to build your code once, using a Linux-style toolchain. The GCC-based compiler provided is itself an Actually Portable Executable, so you can build your software for all six OSes from the comfort of whichever one you prefer most for development.\n",
    "\n",
    "4. The weights for an LLM can be embedded within the Llamafile. Mozilla added support for PKZIP to the [GGML](https://github.com/ggerganov/ggml) library. This lets uncompressed weights be mapped directly into memory, similar to a self-extracting archive. It enables quantized weights distributed online to be prefixed with a compatible version of the llama.cpp software, thereby ensuring its originally observed behaviors can be reproduced indefinitely.\n",
    "\n",
    "5. Finally, with the tools included in this project you can create your *own* llamafiles, using any compatible model weights you want. You can then distribute these llamafiles to other people, who can easily make use of them regardless of what kind of computer they have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52722c-5943-4da4-b91e-8eb1b8161491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
