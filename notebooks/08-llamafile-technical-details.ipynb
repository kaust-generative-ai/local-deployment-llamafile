{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d02447-0757-4382-a510-943e46a7a473",
   "metadata": {},
   "source": [
    "## Technical details\n",
    "\n",
    "Here is a succinct overview of the tricks we used to create the fattest\n",
    "executable format ever. The long story short is llamafile is a shell\n",
    "script that launches itself and runs inference on embedded weights in\n",
    "milliseconds without needing to be copied or installed. What makes that\n",
    "possible is mmap(). Both the llama.cpp executable and the weights are\n",
    "concatenated onto the shell script. A tiny loader program is then\n",
    "extracted by the shell script, which maps the executable into memory.\n",
    "The llama.cpp executable then opens the shell script again as a file,\n",
    "and calls mmap() again to pull the weights into memory and make them\n",
    "directly accessible to both the CPU and GPU.\n",
    "\n",
    "### ZIP weights embedding\n",
    "\n",
    "The trick to embedding weights inside llama.cpp executables is to ensure\n",
    "the local file is aligned on a page size boundary. That way, assuming\n",
    "the zip file is uncompressed, once it's mmap()'d into memory we can pass\n",
    "pointers directly to GPUs like Apple Metal, which require that data be\n",
    "page size aligned. Since no existing ZIP archiving tool has an alignment\n",
    "flag, we had to write about [500 lines of code](llamafile/zipalign.c) to\n",
    "insert the ZIP files ourselves. However, once there, every existing ZIP\n",
    "program should be able to read them, provided they support ZIP64. This\n",
    "makes the weights much more easily accessible than they otherwise would\n",
    "have been, had we invented our own file format for concatenated files.\n",
    "\n",
    "### Microarchitectural portability\n",
    "\n",
    "On Intel and AMD microprocessors, llama.cpp spends most of its time in\n",
    "the matmul quants, which are usually written thrice for SSSE3, AVX, and\n",
    "AVX2. llamafile pulls each of these functions out into a separate file\n",
    "that can be `#include`ed multiple times, with varying\n",
    "`__attribute__((__target__(\"arch\")))` function attributes. Then, a\n",
    "wrapper function is added which uses Cosmopolitan's `X86_HAVE(FOO)`\n",
    "feature to runtime dispatch to the appropriate implementation.\n",
    "\n",
    "### Architecture portability\n",
    "\n",
    "llamafile solves architecture portability by building llama.cpp twice:\n",
    "once for AMD64 and again for ARM64. It then wraps them with a shell\n",
    "script which has an MZ prefix. On Windows, it'll run as a native binary.\n",
    "On Linux, it'll extract a small 8kb executable called [APE\n",
    "Loader](https://github.com/jart/cosmopolitan/blob/master/ape/loader.c)\n",
    "to `${TMPDIR:-${HOME:-.}}/.ape` that'll map the binary portions of the\n",
    "shell script into memory. It's possible to avoid this process by running\n",
    "the\n",
    "[`assimilate`](https://github.com/jart/cosmopolitan/blob/master/tool/build/assimilate.c)\n",
    "program that comes included with the `cosmocc` compiler. What the\n",
    "`assimilate` program does is turn the shell script executable into\n",
    "the host platform's native executable format. This guarantees a fallback\n",
    "path exists for traditional release processes when it's needed.\n",
    "\n",
    "### GPU support\n",
    "\n",
    "Cosmopolitan Libc uses static linking, since that's the only way to get\n",
    "the same executable to run on six OSes. This presents a challenge for\n",
    "llama.cpp, because it's not possible to statically link GPU support. The\n",
    "way we solve that is by checking if a compiler is installed on the host\n",
    "system. For Apple, that would be Xcode, and for other platforms, that\n",
    "would be `nvcc`. llama.cpp has a single file implementation of each GPU\n",
    "module, named `ggml-metal.m` (Objective C) and `ggml-cuda.cu` (Nvidia\n",
    "C). llamafile embeds those source files within the zip archive and asks\n",
    "the platform compiler to build them at runtime, targeting the native GPU\n",
    "microarchitecture. If it works, then it's linked with platform C library\n",
    "dlopen() implementation. See [llamafile/cuda.c](llamafile/cuda.c) and\n",
    "[llamafile/metal.c](llamafile/metal.c).\n",
    "\n",
    "In order to use the platform-specific dlopen() function, we need to ask\n",
    "the platform-specific compiler to build a small executable that exposes\n",
    "these interfaces. On ELF platforms, Cosmopolitan Libc maps this helper\n",
    "executable into memory along with the platform's ELF interpreter. The\n",
    "platform C library then takes care of linking all the GPU libraries, and\n",
    "then runs the helper program which longjmp()'s back into Cosmopolitan.\n",
    "The executable program is now in a weird hybrid state where two separate\n",
    "C libraries exist which have different ABIs. For example, thread local\n",
    "storage works differently on each operating system, and programs will\n",
    "crash if the TLS register doesn't point to the appropriate memory. The\n",
    "way Cosmopolitan Libc solves that on AMD is by using SSE to recompile\n",
    "the executable at runtime to change `%fs` register accesses into `%gs`\n",
    "which takes a millisecond. On ARM, Cosmo uses the `x28` register for TLS\n",
    "which can be made safe by passing the `-ffixed-x28` flag when compiling\n",
    "GPU modules. Lastly, llamafile uses the `__ms_abi__` attribute so that\n",
    "function pointers passed between the application and GPU modules conform\n",
    "to the Windows calling convention. Amazingly enough, every compiler we\n",
    "tested, including nvcc on Linux and even Objective-C on MacOS, all\n",
    "support compiling WIN32 style functions, thus ensuring your llamafile\n",
    "will be able to talk to Windows drivers, when it's run on Windows,\n",
    "without needing to be recompiled as a separate file for Windows. See\n",
    "[cosmopolitan/dlopen.c](https://github.com/jart/cosmopolitan/blob/master/libc/dlopen/dlopen.c)\n",
    "for further details.\n",
    "\n",
    "## Security\n",
    "\n",
    "llamafile adds pledge() and SECCOMP sandboxing to llama.cpp. This is enabled by default. It can be turned off by passing the `--unsecure` flag. Sandboxing is currently only supported on Linux and OpenBSD on systems without GPUs; on other platforms it'll simply log a warning.\n",
    "\n",
    "Our approach to security has these benefits:\n",
    "\n",
    "1. After it starts up, your HTTP server isn't able to access the filesystem at all. This is good, since it means if someone discovers a bug in the llama.cpp server, then it's much less likely they'll be able to access sensitive information on your machine or make changes to its configuration. On Linux, we're able to sandbox things even further; the only networking related system call the HTTP server will allowed to use after starting up, is accept(). That further limits an attacker's ability to exfiltrate information, in the event that your HTTP server is compromised.\n",
    "\n",
    "2. The main CLI command won't be able to access the network at all. This is enforced by the operating system kernel. It also won't be able to write to the file system. This keeps your computer safe in the event that a bug is ever discovered in the GGUF file format that lets an attacker craft malicious weights files and post them online. The only exception to this rule is if you pass the `--prompt-cache` flag without also specifying `--prompt-cache-ro`. In that case, security currently needs to be weakened to allow `cpath` and `wpath` access, but network access will remain forbidden.\n",
    "\n",
    "Therefore your llamafile is able to protect itself against the outside world, but that doesn't mean you're protected from llamafile. Sandboxing is self-imposed. If you obtained your llamafile from an untrusted source then its author could have simply modified it to not do that. In that case, you can run the untrusted llamafile inside another sandbox, such as a virtual machine, to make sure it behaves how you expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52722c-5943-4da4-b91e-8eb1b8161491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
