{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd55d257-12d1-4d08-965e-e5cda7a2ae3e",
   "metadata": {},
   "source": [
    "## GPU support\n",
    "\n",
    "llamafile supports the following kinds of GPUs:\n",
    "\n",
    "- Apple Metal\n",
    "- NVIDIA\n",
    "- AMD\n",
    "\n",
    "GPU on MacOS ARM64 is supported by compiling a small module using the Xcode Command Line Tools, which need to be installed. This is a one time cost that happens the first time you run your llamafile. The DSO built by llamafile is stored in `$TMPDIR/.llamafile` or `$HOME/.llamafile`. Offloading to GPU is enabled by default when a Metal GPU is present.\n",
    "This can be disabled by passing `-ngl 0` or `--gpu disable` to force llamafile to perform CPU inference.\n",
    "\n",
    "Owners of NVIDIA and AMD graphics cards need to pass the `-ngl 999` flag to enable maximum offloading. If multiple GPUs are present then the work will be divided evenly among them by default, so you can load larger models. Multiple GPU support may be broken on AMD Radeon systems. If that happens to you, then use `export HIP_VISIBLE_DEVICES=0` which forces llamafile to only use the first GPU.\n",
    "\n",
    "Windows users are encouraged to use our release binaries, because they contain prebuilt DLLs for both NVIDIA and AMD graphics cards, which only depend on the graphics driver being installed. If llamafile detects that NVIDIA's CUDA SDK or AMD's ROCm HIP SDK are installed, then llamafile will try to build a faster DLL that uses cuBLAS or rocBLAS. In order for\n",
    "llamafile to successfully build a cuBLAS module, it needs to be run on the x64 MSVC command prompt. You can use CUDA via WSL by enabling [Nvidia CUDA on WSL](https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl) and running your llamafiles inside of WSL. Using WSL has the added benefit of letting you run llamafiles greater than 4GB on Windows.\n",
    "\n",
    "On Linux, NVIDIA users will need to install the CUDA SDK (ideally using the shell script installer) and ROCm users need to install the HIP SDK. They're detected by looking to see if `nvcc` or `hipcc` are on the PATH.\n",
    "\n",
    "If you have both an AMD GPU *and* an NVIDIA GPU in your machine, then you may need to qualify which one you want used, by passing either `--gpu amd` or `--gpu nvidia`.\n",
    "\n",
    "In the event that GPU support couldn't be compiled and dynamically linked on the fly for any reason, llamafile will fall back to CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52722c-5943-4da4-b91e-8eb1b8161491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
