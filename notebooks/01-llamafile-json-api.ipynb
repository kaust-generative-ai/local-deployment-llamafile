{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54bc67b-a2bd-417d-889c-1d389c1422b2",
   "metadata": {},
   "source": [
    "## JSON API Quickstart\n",
    "\n",
    "When llamafile is started, in addition to hosting a web UI chat server at <http://127.0.0.1:8080/>, an [OpenAI\n",
    "API](https://platform.openai.com/docs/api-reference/chat) compatible chat completions endpoint is provided too. It's designed to support the most common OpenAI API use cases, in a way that runs entirely locally.\n",
    "\n",
    "We've also extended it to include llama.cpp specific features (e.g.mirostat) that may also be used. For further details on what fields and endpoints are available, refer to both the [OpenAI\n",
    "documentation](https://platform.openai.com/docs/api-reference/chat/create) and the [llama.cpp server\n",
    "README](https://github.com/ggerganov/llama.cpp/tree/master/examples/server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b228c-2313-4204-82e3-2bfb2540607a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae01dc5-377c-4f2f-95fd-949d9b2dc563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
